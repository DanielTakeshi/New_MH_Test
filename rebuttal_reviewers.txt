We thank the reviewers for their thoughtful commentary on our paper to keep the quality of NIPS
top-notch. We start the rebuttal with a few general comments and then respond to specific reviewers.


General comments:

We have been working continuously on the paper in the last month, and have been able to address a
number of our reviewer comments. For instance, we have been able to demonstrate worst-case scenarios
for references [3] and [4] with Omega(N) lower bounds for fixed proposals using Gaussians. We have
also added rigorous, numerical error bounds based on t-statistics (from the empirical mean and
variance) asymptotics from [13]. We have added bounds on the equilibrium distribution given bounds
on transition kernel error (we will borrow this from [4]) with the appropriate assumptions. 
(This section addresses theoretical results)


To Reviewers 2, 3, and 4 regarding the Barker function:

You are correct, the logistic function (aka Barker function) is less efficient with \theta' as good
as \theta. However, the tradeoff is that using the Barker function lets us better control our error
rates, and our newest experiments show that we are obtaining orders of magnitude improvements.


To Reviewers 1, 2, and 3 regarding the convolution (in)stability:

To make it more stable, we can use L2 regularization and move some of the variance from X_norm to
X_corr. i.e. widen X_corr by convolving with a Gaussian with variance sigma^2, while decreasing the
allowed variance of X_norm by sigma^2. This ensures that X_corr has positive pdf everywhere.  We can
achieve CDF error (X_norm + X_corr compared to X_log) of 1e-7 at sigma = 0.8 or 1e-5 at sigma = 0.9.
We are substantially reworking this part of the paper.


To Reviewer 2

(TODO?)


To Reviewers 1 and 3 regarding Lemma 2:

Both of you have correctly identified our implicit assumptions about Lemma 2. Reviewer_1 is right
that we approximated it for large N, so sampling without replacement approximates sampling with
replacement.  Reviewer_3 is correct in that the CLT only applies for large n and N, and we also need
n << N. Note also that we generally assume n is in the several hundreds, which is often enough for
the CLT to hold. We will clarify this in our revisions.


To Reviewers 1 and 2:

Thank you for mentioning [BDH], "MCMC Methods for Tall Data." We added this reference as part of our
ongoing revision of the paper and will substantially address issues raised in [BDH]. We are also
addressing Reviewer 2's other references.  In addition, we are addressing Reviewer 2's concern
regarding the difference between ours and [BDH] Section 6.3. Yes, it's close to what we are doing,
but they don't attempt to correct for the difference between normal and logistic distributions with
an additive variable, so they are stuck with a fixed error.


To Reviewer 7:

We should have clarified, that we have the option of increasing minibatch sizes, and the idea is
that we can start by increasing it just enough for the variance condition to apply. Once we have
identified a suitable minibatch size, we keep it fixed thereafter. In our latest revisions, we rerun
our test cases and experiment with reducing the proposal variance or increasing the temperature. By
doing so, we can run our algorithm using arbitrarily small minibatch sizes.
