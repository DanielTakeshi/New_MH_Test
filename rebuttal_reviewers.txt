We thank the reviewers for their thoughtful commentary on our paper to keep the quality of NIPS
top-notch. We start the rebuttal with a few general comments and then respond to specific reviewers.


General:

We have been working continuously on the paper in the last month, and have been able to address a
number of our reviewer comments. First, we have demonstrated worst-case scenarios for references [3]
and [4] with Omega(N) lower bounds on the number of samples consumed per iteration, using simple
Gaussian examples. 

In addition, Section 4 (the theoretical results) has been vastly revamped.  We have added rigorous,
numerical error bounds based on t-statistics (from the empirical mean and variance) asymptotics from
[13]. We have added bounds on the equilibrium distribution given bounds on transition kernel error
(we will borrow this from [4]) with the appropriate assumptions.


To Reviewers 2, 3, and 4 regarding the Barker function:

You are correct, the logistic (Barker) function is less efficient with \theta' as good as \theta,
and we explicitly mention this in our revision. The tradeoff is that its smoothness allows it to
naturally tolerate substantial variance in its input argument. This in turn will lead to a much more
efficient test on subsets of data, and our newest experiments show we have more control over error
rates.


To Reviewers 1, 2, and 3 regarding the convolution (in)stability:

To make the convolution more stable, we frame it as a least squares (with L2 regularization)
minimization problem. We move some of the variance from X_norm to X_corr, i.e. widen X_corr by
convolving with a Gaussian with variance sigma^2, while decreasing the allowed variance of X_norm by
sigma^2. This ensures that X_corr has positive pdf everywhere.  We can achieve CDF error (X_norm +
X_corr compared to X_log) of 1e-7 at sigma = 0.8 or 1e-5 at sigma = 0.9.  We have added an entire
new section explaining this procedure.


To Reviewer 3:

(TODO respond to experiment criticism, but I really can't defend the work as it is ...)


To Reviewers 1 and 3 regarding Lemma 2:

Both of you have correctly identified our implicit assumptions about Lemma 2. Reviewer_1 is right
that we approximated it for large N, so sampling without replacement approximates sampling with
replacement.  Reviewer_3 is correct in that the CLT only applies for large n and N, and we also need
n << N. Note also that we generally assume n is in the several hundreds, which is often enough for
the CLT to hold. We will clarify this in our revisions.


To Reviewers 1 and 2:

Thank you for mentioning [BDH], "MCMC Methods for Tall Data." We added this reference as part of our
ongoing revision of the paper and will substantially address issues raised in [BDH]. We are also
addressing Reviewer 2's other references.  In addition, we are addressing Reviewer 2's concern
regarding the difference between ours and [BDH] Section 6.3. Yes, it's close to what we are doing,
but they don't attempt to correct for the difference between normal and logistic distributions with
an additive variable, so they are stuck with a fixed error.


To Reviewer 7:

We should have clarified, that we have the option of increasing minibatch sizes, and the idea is
that we can start by increasing it just enough for the variance condition to apply. Once we have
identified a suitable minibatch size, we keep it fixed thereafter. In our latest revisions, we rerun
our test cases and experiment with reducing the proposal variance or increasing the temperature. By
doing so, we can run our algorithm using arbitrarily small minibatch sizes.
