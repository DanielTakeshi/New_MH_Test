\documentclass[twoside]{article} \usepackage{aistats2017}

% If your paper is accepted, change the options for the package
% aistats2017 as follows:
%
%\usepackage[accepted]{aistats2017}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.
\usepackage{amsmath,amsthm,color,graphicx,verbatim,listings,enumitem,float}
\usepackage{amssymb}
\usepackage{natbib}  % for references
\usepackage{amsfonts}
\usepackage{nicefrac}  
\usepackage{booktabs}
\usepackage{microtype}
\usepackage[]{algorithm2e}
\usepackage{caption}
\usepackage{subcaption}
\graphicspath{{figures/}}
\usepackage{setspace}
\lstset{
numbers=left, 
numberstyle=\small, 
numbersep=8pt, 
frame = single, 
language=matlab, 
framexleftmargin=20pt}

\renewcommand{\refname}{}
\newcommand{\simiid}{\overset{\textrm{i.i.d.}}{\sim}}
\DeclareMathOperator*{\argmin}{arg\,min}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

% This portion of style definition will remove the index number before the 
% without inducing an indent
\makeatletter
\renewcommand\@biblabel[1]{}
\renewenvironment{thebibliography}[1]
     {\section*{\refname}%
      \@mkboth{\MakeUppercase\refname}{\MakeUppercase\refname}%
      \list{}%
           {\leftmargin0pt
            \@openbib@code
            \usecounter{enumiv}}%
      \sloppy
      \clubpenalty4000
      \@clubpenalty \clubpenalty
      \widowpenalty4000%
      \sfcode`\.\@m}
     {\def\@noitemerr
       {\@latex@warning{Empty `thebibliography' environment}}%
      \endlist}
\makeatother

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{An Efficient Minibatch Acceptance Test for Metropolis-Hastings}

\aistatsauthor{ Anonymous Author 1 \And Anonymous Author 2 \And Anonymous Author 3 }

\aistatsaddress{ Unknown Institution 1 \And Unknown Institution 2 \And Unknown Institution 3 } ]

\begin{abstract}
  %The Abstract paragraph should be indented 0.25 inch (1.5 picas) on
  %both left and right-hand margins. Use 10~point type, with a vertical
  %spacing of 11~points. The {\bf Abstract} heading must be centered,
  %bold, and in point size 12. Two line spaces precede the
  %Abstract. The Abstract must be limited to one paragraph.
  Markov chain Monte Carlo (MCMC) methods have many applications in
  machine learning. We are particularly interested in their application
  to modeling very large datasets, where it is impractical to perform
  Metropolis-Hastings tests on the full data. Previous work on reducing
  the cost of Metropolis-Hastings tests yield variable data consumed per
  sample, with only constant factor reductions versus using the full
  dataset for each sample.  Here we present a method that can be tuned
  to provide arbitrarily small batch sizes, by adjusting either proposal
  step size or temperature. Our approach uses the natural noise present
  in minibatch likelihood estimates to furnish the randomness in a
  Metropolis-Hastings test. Our test uses the noise-tolerant Barker
  acceptance test with a novel additive correction variable.  The
  resulting test can be combined with minibatch proposals to yield
  updates with the same complexity as a simple SGD update. In this paper
  we derive the test, analyze its performance, discuss its
  implementation, and present several experiments. We demonstrate 
  several order-of-magnitude speedup over previous work, and show for
  the first time that small expected minibatch sizes are possible. 
\end{abstract}

\section{INTRODUCTION}\label{sec:introduction}

Markov chain Monte Carlo (MCMC) sampling is a powerful method for computation on
intractable distributions. We are interested in large dataset applications,
where the goal is to sample a posterior distribution $p(\theta \mid x_1, \ldots,
x_N)$ of parameter $\theta$ for large $N$.  The Metropolis-Hastings method (M-H)
generates sample candidates from a proposal distribution $q$ which is in general
different from the target distribution $p$, and decides whether to accept or
reject based on an acceptance test. The acceptance test is usually a Metropolis
test~\citep{Metropolis1953, hastings70}.

Many state-of-the-art machine learning methods, and deep learning in particular,
are based on minibatch updates (such as SGD) to a model.  Minibatch updates
produce many improvements to the model for each pass over the dataset, and have
high sample efficiency.  In contrast, conventional M-H requires calculations
over the full dataset to produce a new sample.  Recent results
from~\citet{cutting_mh_2014} and~\citet{icml2014c1_bardenet14} perform
approximate (bounded error) acceptance tests using subsets of the full dataset.
The amount of data consumed for each test varies significantly from one
minibatch to the next. By contrast,~\citet{conf/uai/MaclaurinA14,TallData15}
perform exact tests but require a lower bound on parameter distribution across
its domain.  The amount of data reduction depends on the accuracy of this bound,
and such bounds are only available for relatively simple distributions.

Here we derive a new test which incorporates the variability in minibatch
statistics as {\em a natural part of the test} and requires less data per
iteration than prior work. We use a Barker test function~\citep{Barker65}, which
makes our test naturally error tolerant. The idea of using a noise-tolerant
Barker's test function was suggested but not explored empirically
in~\citet{TallData15} section 6.3. But the asymptotic test statistic CDF and the
Barker function are different, which leads to fixed errors for the approach
in~\citet{TallData15}. Here, we show that the difference between the
distributions can be corrected with an additive random variable. This leads to a
test which is fast, and whose error can be made arbitrarily small.

Our test is applicable when the variance (over data samples) of the log
acceptance probability is small enough (less than 1). It's not clear at first why
this quantity should be bounded, but we will show that it is ``natural'' for
well-specified models running Metropolis-Hastings sampling with optimal
proposals~\citep{OptimalScaling01} on a full dataset. When we reduce the amount
of data for the test, the variance goes up. We have to reduce variance in one
of several ways. Either:

\begin{itemize}[noitemsep]
    \item Increase the temperature of the target distribution. Log likelihoods
    scale as $1/T$, and so the variance of the likelihood ratio will vary as
    $1/T^2$. Our model is no longer well-specified (we are doing inference at a
    temperature different from that assumed during data generation), but higher
    temperature can be advantageous for parameter exploration.

    \item Increase the minibatch size when needed. Log acceptance variance
    scales as $1/k$ vs the minibatch size $k$. Our test is adaptive like earlier
    works, but unlike them, the distribution of minibatch size is Gaussian, not
    long-tailed.  Increased minibatch size also reduces the error rate for the
    test.

    \item For continuous distributions, reduce the proposal step size and
    variance compared to an optimal proposal. The variance of the log acceptance
    probability scales as the square of proposal step size. 
\end{itemize}

It is worth discussing at this point the typical goals of M-H sampling on large
datasets. By the Bernstein-von Mises Theorem, the posterior distribution for a
Bayesian inference task has variance that scales inversely with $N$. Simply
sampling from it is one application, but an efficient
proposal~\citep{OptimalScaling01} has similar variance to the target and will
diffuse to it extremely slowly. For applications to neural networks or models
where the posterior is multimodal~\citep{choromanska2014loss}, samplers will
likely get trapped in one of the modes. A common solution is to anneal the
sampler, running first at high temperatures to flatten the likelihood landscape.
This in turn reduces the variance of the log acceptance probability and allows
our test to be applied. Our samples can cover the search space densely with
small steps rather than taking a few sparse steps towards an optimum. In this
mode, Metropolis-Hastings can be used in similar fashion to Stochastic Gradient
Descent. The goal in SGD is to make gradual progress to a posterior mode with
each step, taking small steps so that the cumulative displacement has
progressively lower variance.

% Daniel: I merged these two paragraphs into one, because these were too long,
% and we also never experiment with the step size. Let me know what you think.

%% It is worth discussing at this point the typical goals of M-H sampling on large
%% datasets.  By the Bernstein-von Mises Theorem, the posterior distribution for a
%% Bayesian inference task is asymptotically normal, and has variance that scales
%% inversely with the $N$ data samples. Simply sampling from this distribution is
%% one application, but an efficient proposal~\citep{OptimalScaling01} has similar
%% variance to the target distribution and will diffuse to it extremely slowly from
%% an initialization value which is (likely to be) many standard deviations away.
%% If there are any other strong modes, it is very likely for the sampler to find
%% one of them and become trapped in it when run at the normal distribution
%% temperature ($T=1$). A common solution is to anneal the sampler, running first
%% at high temperature (scaling log likelihoods by $1/T$) which flattens the
%% likelihood landscape.  This in turn reduces the variance of the log acceptance
%% probability and allows our acceptance test to be applied.
%% 
%% A second question concerns step size. Once we have fixed temperature, our
%% variance constraint implies that we have to trade-off proposal step size $s$ and
%% batch size $b$ ($b \propto p^2$), i.e. we can make many small steps, or one
%% large step, with a given batch of data. One of the primary drivers of this work
%% is our belief in the value of small steps. For applications to neural networks
%% or other models where the posterior is multimodal, posterior inference is
%% arguably a search process. Covering the search space densely with small steps is
%% much more valuable than few sparse steps toward the nearest optimum. In this
%% mode, Metropolis-Hastings can be used in similar fashion to Stochastic Gradient
%% Descent. The goal in SGD is to make gradual progress to a posterior mode with
%% each step, taking small steps so that the cumulative displacement has
%% progressively lower variance. We demonstrate this behavior in our logistic
%% regression experiments. 

The contributions of this paper are as follows:

\begin{itemize}[noitemsep]
    \item We develop a new, more efficient (in samples per test) minibatch
    acceptance test with quantifiable error bounds. The test uses a novel
    additive correction variable to implement a Barker test based on minibatch
    mean and variance. 

    %\item We analyze the test for accuracy and speed.

    \item We compare performance of our new test and prior approaches on several
    datasets. We demonstrate orders of magnitude improvements in efficiency
    (measured as data consumed per test), and that it does not suffer from
    long-tailed minibatch sizes.
\end{itemize}



\section{PRELIMINARIES}\label{sec:related_work}

In the Metropolis-Hastings method~\citep{gilks1996markov,brooks2011handbook}, a
difficult-to-compute probability distribution $p(\theta)$ is sampled using a
Markov chain $\theta_1,\ldots,\theta_n$. The sample $\theta_{t+1}$ at time $t+1$
is generated using a candidate $\theta'$ from a (simpler) proposal distribution
$q(\theta'\mid \theta_t)$, filtered by an acceptance test. The acceptance test
is usually a Metropolis test. The Metropolis test has acceptance probability:
\begin{equation}\label{eq:traditional}
    \alpha(\theta_t,\theta') = \frac{p(\theta')q(\theta_t \mid \theta')}{p(\theta_t)q(\theta' \mid \theta_t)} \wedge 1
\end{equation}
where $a \wedge b$ denotes $\min(a,b)$.  With probability
$\alpha(\theta_t,\theta')$, we accept $\theta'$ and set $\theta_{t+1} =
\theta'$, otherwise set $\theta_{t+1}=\theta_t$.  The test is often implemented
with an auxiliary random variable $u \sim \mathcal{U}(0,1)$ with a comparison
$u<\alpha(\theta_t,\theta')$; here, $\mathcal{U}(a,b)$ denotes the uniform
distribution on the interval $[a,b]$.  For simplicity, we drop the subscript $t$
for the current sample $\theta_t$ and denote it as $\theta$. 

The acceptance test guarantees detailed balance, which means
$p(\theta)p(\theta'\mid\theta) = p(\theta')p(\theta \mid\theta')$, where
$p(\theta'\mid\theta)$ is the probability of a transition from state $\theta$ to
$\theta'$. Here, $p(\theta'\mid\theta)=q(\theta'\mid\theta)\alpha(\theta,\theta')$.
% Daniel: I commented this out because people should know what it means and we
%re-state another detailed balance equation later!
%\begin{equation}\label{eq:detailed_balance2}
%    p(\theta)q(\theta'\mid\theta)\alpha(\theta,\theta') = p(\theta')q(\theta\mid\theta')\alpha(\theta',\theta)
%\end{equation}
This condition, together with ergodicity, guarantees that the Markov chain has a
unique stationary distribution $\pi(\theta) = p(\theta)$. For Bayesian
inference, the target distribution is $p(\theta \mid x_1, \ldots, x_N)$. The
acceptance probability is now:
\begin{equation}\label{eq:acceptance_probability}
    \alpha(\theta,\theta') = 
    \frac{p_0(\theta')\prod_{i=1}^N p(x_i \mid \theta')q(\theta \mid
    \theta')}{p_0(\theta)\prod_{i=1}^N p(x_i \mid \theta)q(\theta' \mid\theta)}
    \wedge 1
\end{equation}
where $p_0(\theta)$ is the prior. Computing samples this way requires all $N$
data points, but this is very expensive for large datasets. To address this
challenge,~\citet{cutting_mh_2014,icml2014c1_bardenet14} perform approximate
Metropolis-Hasting tests using sequential hypothesis testing. Each iteration,
they start with a small minibatch and test whether $\theta'$ should be accepted
based on approximating $u<\alpha(\theta,\theta')$. If the approximate test
cannot decide with sufficient confidence, the minibatch size is increased and
the test repeats. This process continues until a decision. The bounds depend on
either an asymptotic Central Limit Theorem~\citep{cutting_mh_2014} or a
concentration bound~\citep{icml2014c1_bardenet14}. The latter requires direct
bounds on the log likelihood ratio, which for general distributions requires
knowing $p(x_i \mid \theta)$ and $p(x_i \mid \theta')$ for all $N$ samples. In
addition, while both methods show useful reductions in the number of samples
required, they suffer the drawback of resolving small log likelihood ratio
differences between the minibatch and full batch versions. We discuss a
worst-case scenario in Section~\ref{ssec:gaussian_example}.

Following~\citet{icml2014c1_bardenet14}, we write the test
$u<\alpha(\theta,\theta')$ equivalently as $\Lambda(\theta,\theta') >
\psi(u,\theta,\theta')$, where\footnote{Our definitions differ from those
in~\citet{icml2014c1_bardenet14} by a factor of $N$ to simplify our analysis
later.}
\begin{equation}
\begin{split} 
\Lambda(\theta,\theta') & = \sum_{i=1}^N \log\left(\frac{p(x_i|\theta')}{p(x_i|\theta)}\right) \\  
{\rm and} \quad \psi(u,\theta,\theta') &= \log\left(u\frac{q(\theta'|\theta)p_0(\theta)}{q(\theta|\theta')p_0(\theta')}\right).
\end{split}
\end{equation}
To reduce computational effort, an unbiased estimate of $\Lambda(\theta,\theta')$
based on a minibatch can be used:
\begin{equation}
\Lambda^*(\theta,\theta') = \frac{N}{b}\sum_{i=1}^b \log\left(\frac{p(x_i|\theta')}{p(x_i|\theta)}\right)  
\end{equation}
Finally, it will be convenient for our analysis to define
$\Lambda_i(\theta,\theta') = N\log(\frac{p(x_i|\theta')}{p(x_i|\theta)})$.
%% \begin{equation}\label{eq:individual_terms}
%% \Lambda_i(\theta,\theta') = N \log\left(\frac{p(x_i|\theta')}{p(x_i|\theta)}\right)  
%% \end{equation}
Thus, $\Lambda(\theta,\theta')$ is the mean of $\Lambda_i(\theta,\theta')$ over
the entire dataset, and $\Lambda^*(\theta,\theta')$ is the mean of
$\Lambda_i(\theta,\theta')$ over its minibatch. 

Since minibatches contains randomly selected samples $x_i$, the values
$\Lambda_i$ are i.i.d. random variables\footnote{The analysis assumes sampling
with replacement although implementations on typical large datasets will
approximate this by sampling without replacement.}.  By the Central Limit
Theorem, we expect $\Lambda^*(\theta,\theta')$ to be approximately Gaussian. The
acceptance test then becomes a statistical test of the hypothesis that
$\Lambda(\theta,\theta')>\psi(u,\theta,\theta')$ by establishing that
$\Lambda^*(\theta,\theta')$ is substantially larger than
$\psi(u,\theta,\theta')$.


\subsection{A Worst-Case Gaussian Example}\label{ssec:gaussian_example}

Let $x_1,\ldots,x_N$ be i.i.d. $\mathcal{N}(\theta,1)$ with known variance
$\sigma^2=1$ and (unknown) mean $\theta=0.5$. We use a uniform prior on
$\theta$. The log likelihood ratio is
\begin{equation}\label{eq:lemma_ll_ratio}
    \Lambda^*(\theta,\theta') = N(\theta'-\theta)\left(\frac{1}{b}\sum_{i=1}^b x_i-\theta-\frac{\theta'-\theta}{2}\right)
\end{equation}
which is normally distributed over selection of the Normal samples $x_i$.  Since
the $x_i$ have unit variance, their mean has variance $1/b$, and the variance of
$\Lambda^*(\theta,\theta')$ is $\sigma^2(\Lambda^*) = (\theta'-\theta)^2N^2/b$.
In order to pass a hypothesis test that $\Lambda > \psi$, there needs to be a
large enough gap (several $\sigma(\Lambda^*)$) between
$\Lambda^*(\theta,\theta')$ and $\psi(u,\theta,\theta')$. 

The posterior is a Gaussian centered on the sample mean $\mu$, and with variance
$1/N$ (i.e., $\mathcal{N}(\mu, 1/N)$). In one dimension, an efficient proposal
distribution has the same variance as the target
distribution~\citep{OptimalScaling01}, so we use a proposal based on
$\mathcal{N}(\theta,1/N)$. It is symmetric
$q(\theta'\mid\theta)=q(\theta\mid\theta')$, and since we assumed a uniform
prior, $\psi(u,\theta,\theta')=\log u$. Our worst-case scenario is specified in
Lemma~\ref{lem:worst_case}.

\begin{lemma}\label{lem:worst_case}
    For the model in Section~\ref{ssec:gaussian_example}, there exists a fixed
    (independent of $N$) constant $c$ such that with probability $\geq c$ over
    the joint distribution of $(\theta, \theta', u)$, the tests
    from~\cite{cutting_mh_2014,icml2014c1_bardenet14} consume all $N$ samples. 
\end{lemma}
\vspace{-1em}
\begin{proof}
See Supplementary Material, Section~\ref{app:worst_case_proof}.
\end{proof}
Similar results can be shown for other distributions and proposals by
identifying regions in product space $(\theta,\theta',u)$ such that the
hypothesis test needs to separate nearly-equal values.  It follows that the
accelerated tests from prior work require at least a constant fraction $\geq c$
in the amount of data consumed per test compared to full-data tests, so their
speed-up is $\le 1/c$. The issue is the use of tail bounds to separate $\Delta$
from zero; for certain input/random $u$ combinations, $\Delta$ can be
arbitrarily close to zero. We avoid this by using the {\em approximately normal}
variation in $\Delta^*$ to {\em replace} the variation due to $u$. 

\subsection{MCMC Posterior Inference}
There is a separate line of MCMC work drawing principles from statistical
physics. One can apply
Hamiltonian Monte Carlo (HMC)~\citep{mcmc_hamiltonian_2010} methods which
generate high acceptance \emph{and} distant proposals when run on full batches
of data. Recently Langevin Dynamics~\citep{langevin_2011,conf/icml/AhnBW12} has
been applied to Bayesian estimation on minibatches of data. This simplified
dynamics uses local proposals and avoids M-H tests by using small proposal steps
whose acceptance approaches 1 in the limit. However, the constraint on proposal
step size is severe, and the state space exploration reduces to a random walk.
Full minibatch HMC for minibatches was described in~\citet{sghmc_2014} which
allows momentum-augmented proposals with larger step sizes. However, step sizes
are still limited by the need to run accurately without M-H tests.  By providing
an M-H test with similar cost to standard gradient steps, our work opens the
door to applying those methods with much more aggressive step sizes without loss
of accuracy. 




\section{A NEW MH ACCEPTANCE TEST}\label{sec:our_algorithm}

\subsection{Log-Likelihood Ratios}\label{ssec:log_likelihood_ratios}

For our new M-H test, we denote the exact and approximate log likelihood ratios
as $\Delta$ and $\Delta^*$, respectively. First, $\Delta$ is defined as
\begin{equation}\label{eq:delta1}
    \Delta(\theta,\theta')  =
    \log \frac{p_0(\theta')\prod_{i=1}^N p(x_i \mid \theta')q(\theta \mid
    \theta')}{p_0(\theta)\prod_{i=1}^N p(x_i \mid \theta)q(\theta' \mid\theta)},
\end{equation}
where $p_0, p$, and $q$ match the corresponding functions within
Equation~(\ref{eq:acceptance_probability}). We separate out terms dependent and
independent of the data $x$ as:
\begin{equation}\label{eq:delta2}
\begin{split}
    \Delta(\theta,\theta') &= \sum_{i=1}^N\log\frac{p(x_i\mid\theta')}{p(x_i\mid\theta)} - \psi(1,\theta,\theta') \\
    & = \Lambda(\theta,\theta') - \psi(1,\theta,\theta').
\end{split}
\end{equation}
A minibatch estimator of $\Delta$, denoted as $\Delta^*$, is
\begin{equation}\label{eq:delta3}
\begin{split}
    \Delta^*(\theta,\theta') &=
\frac{N}{b}\sum_{i=1}^b\log\frac{p(x_i\mid\theta')}{p(x_i\mid\theta)} - \psi(1,\theta,\theta')\\
&=\Lambda^*(\theta,\theta') - \psi(1,\theta,\theta')
\end{split}
\end{equation}
Note that $\Delta$ and $\Delta^*$ are evaluated on the full dataset and a
minibatch of size $b$ respectively. The term $N/b$ means
$\Delta^*(\theta,\theta')$ is an unbiased estimator of $\Delta(\theta,\theta')$.

The key to our test is a smooth acceptance function.  We consider functions
other than the classical Metropolis test that satisfy the detailed balance
condition needed for accurate posterior estimation. A class of suitable
functions is specified as follows:

\begin{lemma}\label{lem:detailed_balance}
    If $g(s)$ is any function such that $g(s) = \exp(s) g(-s)$, then the
    acceptance function $\alpha(\theta,\theta') \triangleq
    g(\Delta(\theta,\theta'))$ satisfies detailed balance.
\end{lemma}

This result is used in~\citet{Barker65} to define the Barker acceptance test.  As
a sanity check, choosing $g(s) = \exp(s) \wedge 1$ --- a function satisfying the
requirement of Lemma~\ref{lem:detailed_balance} --- produces the classical
Metropolis acceptance test $\alpha(\theta,\theta') = g(\Delta(\theta,\theta')) =
\frac{p(\theta')q(\theta \mid \theta')}{p(\theta)q(\theta' \mid \theta)}\wedge
1$. In fact, $g(s) =\exp(s) \wedge 1$ is the optimal acceptance function in
terms of acceptance rate, since it accepts with probability 1 for $\Delta > 0$.

\subsection{Barker (Logistic) Acceptance Function}\label{ssec:barker_function}
For our new MH test we use the Barker logistic~\citep{Barker65} function:
$g(s)=(1+\exp(-s))^{-1}$. Straightforward arithmetic shows that it satisfies the
condition in Lemma~\ref{lem:detailed_balance}.  While it is slightly less
efficient than the Metropolis test when used on the full dataset, we will see
that its smoothness allows it to naturally tolerate substantial variance in its
input. This in turn will lead to a much more efficient test on subsets of data.

Assume we begin with the current sample $\theta$ and a candidate sample
$\theta'$, and that $V \sim \mathcal{U}(0,1)$ is a uniform random variable. We
accept $\theta'$ if $g(\Delta(\theta,\theta')) > V$, and reject otherwise.
Since $g(s)$ is monotonically increasing, its inverse $g^{-1}(s)$ is
well-defined and unique. So an equivalent test is to accept $\theta'$ iff
\begin{equation}\label{eq:equivalent_test}
    \Delta(\theta,\theta') > X = g^{-1}(V)
\end{equation}
where $X$ is a random variable with the logistic distribution (its CDF is the
logistic function). To see this notice that $\frac{dV}{dX} = g'$, that $g'$ is
the density corresponding to a logistic CDF, and finally that $\frac{dV}{dX}$ is
the density of $X$. The density of $X$ is symmetric, so we can equivalently test
whether
\begin{equation}\label{eq:the_exact_test}
    \Delta(\theta,\theta') + X > 0
\end{equation}
for a logistic random variable $X$.


\subsection{A Minibatch Acceptance Test}\label{ssec:deltas_minibatch}

We now describe acceptance testing using the minibatch estimator
$\Delta^*(\theta,\theta')$. From Equation~(\ref{eq:delta3}),
$\Delta^*(\theta,\theta')$ can be represented as a constant term plus the mean
of $b$ IID terms $\Lambda_i(\theta,\theta')$ of the form
$N\log\frac{p(x_i|\theta')}{p(x_i|\theta)}$. As $b$ increases,
$\Delta^*(\theta,\theta')$ therefore has a distribution which approaches a
normal distribution by the Central Limit Theorem. We now describe this using an
asymptotic argument and defer specific bounds between the CDFs of
$\Delta^*(\theta,\theta')$ and a Gaussian to Section~\ref{sec:analysis}.

In the limit, since $\Delta^*$ is normally distributed about its mean $\Delta$,
we can write
\begin{equation}\label{eq:relationship}
    \Delta^* = \Delta + X_{\rm norm}, \quad X_{\rm norm} \sim \mathcal{\bar{N}}(0, \sigma^2(\Delta^*)),
\end{equation}
where $\mathcal{\bar{N}}(0, \sigma^2(\Delta^*))$ denotes a distribution which is
approximately normal with variance $\sigma^2(\Delta^*)$.  But to perform the
test in Equation~(\ref{eq:the_exact_test}) we want $\Delta + X$ for a logistic
random variable $X$ (call it $X_{\rm log}$ from now on). In~\citet{TallData15} it
was proposed to use $\Delta^*$ in a Barker test anyway and tolerate the fixed
error caused by this approximation. 

Our approach is to instead decompose $X_{\rm log}$ as
\begin{equation}\label{eq:deconvolution}
    X_{\rm log} = X_{\rm norm}+X_{\rm corr},
\end{equation}
where we assume $X_{\rm norm} \sim \mathcal{N}(0, \sigma^2)$ and that $X_{\rm
corr}$ is a zero-mean ``correction'' variable with density $C_{\sigma}(X)$.  The
two variables are added (i.e., their distributions convolve) to form $X_{\rm
log}$.  This decomposition requires an appropriate $C_\sigma$, which we derive
in Section~\ref{sec:correction}. Using $X_{\rm corr}$ samples from
$C_{\sigma}(X)$, the acceptance test is now
\begin{equation}\label{eq:criteria}
    \Delta + X_{\rm log} = (\Delta + X_{\rm norm}) + X_{\rm corr} = \Delta^* + X_{\rm corr} >0.
\end{equation}
Therefore, assuming the variance of $\Delta^*$ is small enough, if we have an
estimate of $\Delta^*$ from the current data minibatch, we test acceptance by
adding a random variable $X_{\rm corr}$ and then accept $\theta'$ if the result
is positive (and reject otherwise).

If $\mathcal{\bar{N}}(0, \sigma^2(\Delta^*))$ is exactly $\mathcal{N}(0,
\sigma^2(\Delta^*))$, the above test is exact, and as we show in
Section~\ref{sec:analysis}, if there is a maximum error $\epsilon$ between the
CDF of $\mathcal{\bar{N}}(0, \sigma^2(\Delta^*))$ and the CDF of $\mathcal{N}(0,
\sigma^2(\Delta^*))$, then our test has an error of at most $\epsilon$ relative
to the full batch version.



% Daniel: Saves space by using one line instead of two. Lame, I know.
%\section{COMPUTING THE CORRECTION DISTRIBUTION}\label{sec:correction}
\section{CORRECTION DISTRIBUTION}\label{sec:correction}

Our test in Equation~(\ref{eq:criteria}) requires knowing the distribution of
$X_{\rm corr}$. In Section~\ref{sec:analysis}, we show that the test accuracy
depends on the absolute error between the CDFs of $X_{\rm norm} + X_{\rm corr}$
and $X_{\rm log}$. Consequently, we need to minimize this in our construction of
$X_{\rm corr}$. More formally, let $\Phi_{s_X} = \Phi(X/s_X)$ where $\Phi$ is
the standard normal CDF\footnote{Hence, $\Phi_{s_X}$ is the CDF of a zero-mean
Gaussian with standard deviation $s_X$.}, $S(X)$ be the logistic function, and
$C_{\sigma}(X)$ be the \emph{density} of the correction $X_{\rm corr}$
distribution. Our goal is to solve:
\begin{equation}\label{eq:overall_corr_problem}
    C_\sigma^* = \argmin_{C_\sigma} |\Phi_{\sigma} * C_{\sigma} - S|
\end{equation}
where $*$ denotes convolution.

For computation of $C_\sigma$, we assume that its input $Y$ and another variable
$X$ lie in the intervals $[-V,V]$ and $[-2V,2V]$, respectively.
% Daniel: removing this because we were explaining this in quite some detail and
% it was taking a lot of space.
%\[
%    \argmin_{C\sigma} \sup_{x \in [-2V,2V]}\left|\int_{-V}^{V}\Phi_{\sigma}(x-y) C_{\sigma}(y)dy - S(x)\right|.
%\]
We discretize the convolution by discretizing $X$ and $Y$ into $4N+1$ and $2N+1$
values respectively. If $i \in \{-2N, \ldots, 2N\}=\mathcal{I}$ and $j \in \{-N,
\ldots, N\}=\mathcal{J}$, then we can write $X_i = i(V/N)$ and $Y_j = j(V/N)$, and the
objective can be written as:
\[
C_\sigma^* = \argmin_{C_\sigma} \max_{i \in \mathcal{I}}\left|\sum_{j\in\mathcal{J}} \Phi_{\sigma}(X_i-Y_j) C_{\sigma}(Y_j) - S(X_i)\right|.
\]
We now define a matrix $M$ and vectors $u$ and $v$ such that $M_{ij} =
\Phi_{\sigma}(X_i-Y_j)$, $u_j = C_{\sigma}(Y_j)$, and $v_i = S(X_i)$, where the
indices $i$ and $j$ are appropriately translated to be non-negative indices for
$M, u,$ and $v$. Thus, the problem is now to minimize $\|Mu-v\|_{\infty}$ with
the constraint that $u > 0$ since it represents a density. We approximate this
with a least squares solution:
\begin{equation}\label{eq:optimization_l2}
    u^* = \argmin_u\; \|Mu-v\|_2^2 + \lambda \|u\|_2^2,
\end{equation}
with regularization $\lambda$. The solution is well-known from the normal
equations ($u^* = (M^TM + \lambda I)^{-1}M^Tv$) and in practice yields an
acceptable $L_{\infty}$ norm.

% Daniel: This was taking up too much space, and we usually don't want to spend
% a paragraph explaining what *didn't* work, and readers will know what it means
% to minimize the L-infinity norm, we can put it inline.
% \begin{equation}\label{eq:optimization_l1}
%     u^* = \argmin_{u} \|Mu-v\|_{\infty}.
% \end{equation}
% We have the additional constraint that $u_j > 0$ for all $j$, since $u$
% represents a density. We first explored optimizing this problem with linear
% programming to find a $u$ minimizing $\|Mu - v\|_\infty$ subject to $u \ge 0$.
% 
% This was tractable up to a few hundred dimensions for $u$.  However, the
% discretization error is bounded by the curvature of the underlying functions
% times the squared quantization step. The curvatures are here slightly less than
% one, i.e.  the errors are of the order of $(V/N)^2$.  Here we chose $V=20$ to
% provide adequate containment of the distributions (the CDFs are extremely close
% to either zero or one outside this range). So with 200 points, we have a
% discretization error of approximately 0.01, which is relatively large.  To yield
% higher resolution and lower error, we switched to a least squares solution:

\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\begin{algorithm}[t]
\Input{Number of samples $T$, minibatch size $m$, error bound $\delta$, pre-computed correction $C_1(X)$ distribution, initial sample $\theta_1$.}
\Output{A chain of $T$ samples $\{\theta_1, \ldots, \theta_T\}$ from $p(\theta)$\;}
\For{$t=\{1, \ldots, T\}$}{
    -Propose a candidate $\theta'$\ from proposal $q(\theta'\mid\theta_t)$\;
    -Draw a minibatch of $m$ points $x_i$, compute $\Delta^*(\theta_t,\theta')$ and sample variance $s^2_{\Delta^*}$\;
    -Estimate moments $E|\Lambda_i-\Lambda|$ and $E|\Lambda_i-\Lambda|^3$ from the sample, and error $\epsilon$ from Corollary~\ref{cor:our_bound_delta_prime}\;
    \While {$s^2_{\Delta^*} \ge 1$ {\bf or} $\epsilon > \delta$}{
    	-Draw $m$ more samples to augment the minibatch, update $\Delta^*$, $s^2_{\Delta^*}$ and $\epsilon$ estimates\;
    }
    -Draw $X_{\rm nc} \sim \mathcal{N}(0,1-s^2_{\Delta^*})$ and $X_{\rm corr}$ from the correction distribution $C_1(X)$\;
        
        \eIf{$\Delta^* + X_{\rm nc} + X_{\rm corr} > 0$}{
            -Accept the candidate, $\theta_{t+1} = \theta'$\;
        }{
            -Reject and re-use the old sample, $\theta_{t+1} = \theta_t$\;
        }
    
}
\caption{Our acceptance test for MCMC.}
\label{alg:our_algorithm}
\end{algorithm}

\begin{table}[t]
    \caption{Error ($L_\infty$) in $X_{\rm norm}+X_{\rm corr}$ versus $X_{\rm log}$}
    \label{tab:xcorr}
    \centering
    \begin{tabular}{l l l l}
    ${\bf N}$ & ${\bf \sigma}$ & ${\bf \lambda}$ & ${\bf L_{\infty}}$ {\bf error} \\
    \hline
    4000 & 0.9 & 1 & 1.0e-4  \\
    4000 & 0.8 & 0.03 & 5.0e-6 \\
    \end{tabular}
\end{table}

With this approach, there is no guarantee that $u^* \geq 0$. However, we have
some flexibility in the choice of $\sigma$ in Equation~(\ref{eq:overall_corr_problem}).
As we decrease the variance of $X_{\rm norm}$, the variance of $X_{\rm corr}$
grows by the same amount and is in fact the result of convolution with a
Gaussian whose variance is the difference.  Thus as $\sigma$ decreases,
$C_\sigma(X)$ grows and approaches the derivative of a logistic function at
$\sigma = 0$. It retains some very weak negative values for $\sigma > 0$ but
removal of those values leads to very small error. Table~\ref{tab:xcorr} shows
that the errors between $X_{\rm norm}+X_{\rm corr}$ and $X_{\rm log}$ can be
made very small, approaching single floating precision (about $10^{-7}$), and
Algorithm~\ref{alg:our_algorithm} describes our procedure. A few points:

\begin{itemize}[noitemsep]
    \item It uses an adaptive step size so as to use the smallest possible
    average minibatch size. Unlike previous work however (and as we show in
    Section~\ref{sec:experiments}) the size distribution is short-tailed.

    \item An additional normal variable $X_{\rm nc}$ is added to $\Delta^*$ to
    produce a variable with unit variance. This is not mathematically necessary,
    but allows us to use a single correction distribution $C_1$ with $\sigma=1$
    for $X_{\rm corr}$, saving on memory footprint.

    \item The sample variance $s^2_{\Delta^*}$ is proportional to
    $\|\theta'-\theta\|_2^2$ whose distribution for Normal proposals is the
    square of a normal variable. 
\end{itemize}



\section{ANALYSIS}\label{sec:analysis}

We now derive error bounds for our M-H test, and for the approximate target
distribution that it generates. From Table~\ref{tab:xcorr}, we know that it is
possible to generate the correction samples $X_{\rm corr}$ with a CDF error
approaching single-precision floating point error. We therefore treat $X_{\rm
corr}$ as a sample from the exact correction distribution and we will not
analyze its errors.

In the most similar prior works,~\citet{cutting_mh_2014} uses asymptotic
arguments based on the CLT to argue that its approximate acceptance test error
tends to zero as batch size increases, but no quantitative bounds are given.
In~\citet{icml2014c1_bardenet14}, explicit bounds are given, but they depend on
bounding:
\begin{equation}\label{eq:bad_bound}
    C_{\theta, \theta'} = \max_{1\leq i\leq N}|\log p(x_i\mid\theta') - \log p(x_i\mid\theta)|.
\end{equation}
Such bounds can be derived efficiently for models such as logistic regression,
but it is unclear how to derive them for a complex model such as a neural
network. In general, since a new $\theta'$ value is obtained each iteration, one
would need to use all the $p(x_i\mid \theta')$ terms\footnote{The sample code
provided by the authors of~\citet{icml2014c1_bardenet14} in fact computes
$C_{\theta,\theta'}$ explicitly for each sample generated, i.e. it traverses the
entire data, thus providing no performance advantage over the complete test.}.
In contrast, we use quantitative forms of the Central Limit Theorem which rely
on measurable statistics from a single minibatch. Thus a sampler using our
approach does not need to see data beyond the current minibatch.

In Section~\ref{ssec:delta_star_distribution}, we present bounds on the absolute
and relative error (in terms of the CDFs) of the distribution of $\Delta^*$ vs.
a Gaussian. We then show in Section~\ref{ssec:preserve_bounds} that these bounds
are preserved after the addition of other random variables (e.g., $X_{\rm nc}$
and $X_{\rm corr}$). It then follows that the acceptance test has the same error
bound.

\subsection{Bounding the Error of $\Delta^*$ from Gaussian}\label{ssec:delta_star_distribution}

We use the following quantitative central-limit result:
  
\begin{lemma}\label{lem:quant_clt}
Let $X_1,\ldots,X_n$ be a set of zero-mean, independent, identically-distributed
random variables with sample mean $\bar{X}$ and sample variance $s^2_X$
where:
\begin{equation}
    \bar{X} = \frac{1}{n}\sum_{i=1}^nX_i, \quad s_X = \frac{1}{n}\left(\sum_{i=1}^n(X_i-\bar{X}^2\right)^{\frac{1}{2}}.
\end{equation}
This implies $t=\bar{X}/s_X$ has an approximate Student's distribution
which approaches a normal distribution in the limit. Then
\begin{equation}\label{eq:clt-bounds}
    \sup_x|{\rm Pr}(t<x) - \Phi(x)| \leq \frac{6.4E|X|^3+2E|X|}{\sqrt{n}}.
\end{equation}
\end{lemma}

\begin{proof}
See Supplementary Material, Section~\ref{app:quant_clt}.
\end{proof}

%% {\color{blue} Daniel: the notation here was (and still is, to a lesser extent)
%% the source of considerable confusion for me. Do we want $\sigma(X)$ or
%% $\sigma(\hat{X})$? I think in Lemma~\ref{lem:quant_clt} we want the former, not
%% the latter. OH ALSO, this section uses $n$ as the full data size, which means
%% before this section, we should use $n$, not $N$, for the full data size, right?
%% The Bardenet paper uses $n$ to represent the full data size.

%% JFC - this bound applies to minibatches, not the full dataset so the size $n$ is
%% deliberately not $N$.

Lemma~\ref{lem:quant_clt} demonstrates that as long as we know the first and
third absolute moments $E|X|$ and $E|X|^3$, we can bound the error of the normal
approximation, which decays as $O(n^{-\frac{1}{2}})$. Making the change of
variables $y = x s_X$, Equation~(\ref{eq:clt-bounds}) becomes
\begin{equation}\label{eq:clt-bounds2}
   \sup_y\left|{\rm Pr}(\bar{X}< y) - \Phi\left(\frac{y}{s_X}\right)\right| \leq \frac{6.4E|X|^3+2E|X|}{\sqrt{n}}
\end{equation}
showing that the distribution of $\bar{X}$ approaches the normal distribution
$\mathcal{N}(0,s_X)$ whose variance is $s_X$,
measured from the sample.

To apply this to our test, let $X_i = \Lambda_i(\theta,\theta') -
\Lambda(\theta,\theta')$, so that the $X_i$ are zero-mean, i.i.d. variables. If
instead of all $n$ samples, we only extract a subset of $b$ samples
corresponding to our minibatch, we can connect $\bar{X}$ with our $\Delta^*$
term:
\begin{equation}\label{eq:x-hat}
    \bar{X} = \Delta^*(\theta,\theta') - \Delta(\theta,\theta'),
\end{equation}
so that $s_X = s_{\Delta^*}$. This results in the following:

\begin{corollary}\label{cor:our_bound_delta_prime}
We can now substitute into Equation~(\ref{eq:clt-bounds2}) and displace by the mean, giving:
\begin{align*}
    \sup_y\left|{\rm Pr}(\Delta^*< y) - \Phi\left(\frac{y-\Delta}{s_{\Delta^*}}\right)\right| &\leq \frac{6.4E|X|^3+2E|X|}{\sqrt{b}} \\
    &= \epsilon(\theta,\theta',b).
\end{align*}
\end{corollary} %\vspace{-10pt}
Corollary~\ref{cor:our_bound_delta_prime} shows that the distribution of
$\Delta^*$ approximates a Normal distribution with mean $\Delta$ and variance
$s^2_{\Delta^*}$. Furthermore, it bounds the error with \emph{estimable
quantities}: both $E|X|$ and $E|X|^3$ can be estimated as means of $|\Lambda_i
- \Lambda|$ and $|\Lambda_i - \Lambda|^3$, respectively, on each minibatch. We
expect this will often be accurate enough on minibatches with hundreds or
thousands of points, but otherwise bootstrap CIs can be computed from those
sequences. Since the bounds are monotone in $E|X|$ and $E|X|^3$, using upper
bootstrap CI limits will provide high-confidence error bounds.



\subsection{Error Bounds are Preserved After Adding Random Variables}\label{ssec:preserve_bounds}

We next relate the CDFs of distributions and show that bounds are preserved
after adding random variables.

\begin{lemma}\label{lem:cdf_bounds}
Let $P(x)$ and $Q(x)$ be two CDFs satisfying
$\sup_x|P(x)-Q(x)|\leq \epsilon$ with $x$ in some real range. Let $R(y)$ be the
{\em density} of another random variable $y$. Let $P'$ be the convolution $P*R$
and $Q'$ be the convolution $Q*R$. Then $P'(z)$ (resp. $Q'(z)$) is the CDF of
sum $z=x+y$ of independent random variables $x$ with CDF $P(x)$ (resp. $Q(x)$)
and y with density $R(y)$.  Then
\begin{equation}
    \sup_x|P'(x)-Q'(x)|\leq \epsilon
\end{equation}
\end{lemma}

\begin{proof}
See Supplementary Material, Section~\ref{app:proof_cdf_bounds}.
\end{proof}

From Lemma~\ref{lem:cdf_bounds}, we have the following Corollary:

\begin{corollary}\label{cor:bounds_preserved}
If $\sup_y|{\rm Pr}(\Delta^* < y) - \Phi(\frac{y-\Delta}{s_{\Delta^*}})|
\leq \epsilon(\theta,\theta',b)$, then
\[
    \sup_y|{\rm Pr}(\Delta^*+X_{\rm nc}+X_{\rm corr} < y) - S(y-\Delta)| \leq \epsilon(\theta,\theta',b)
\]
where $S(x)$ is the standard logistic function, and $X_{\rm nc}$ and $X_{\rm corr}$ are generated as per Algorithm 1. 
\end{corollary}

\begin{proof}
See Supplementary Material, Section~\ref{app:bounds_preserved}.
\end{proof}

From Section~\ref{sec:our_algorithm}, as the distribution of $\Delta^*$
approaches a Gaussian, our new MH test becomes more accurate.
Corollary~\ref{cor:bounds_preserved} shows that the bounds from
Section~\ref{ssec:delta_star_distribution} are preserved after the addition of
the random variables we use, showing that our test should remain accurate.

% Daniel: I'm moving this paragraph over here and putting the rest of it in the
% supplementary material since our code doesn't do this, so we should not
% describe this in much detail.
In fact we can do better than $O(n^{-1/2})$ error for the quantitative CLT
(showing the error decreases as $O(n^{-1})$) by using a more precise limit
distribution under an additional assumption. We review the details in
the Supplementary Material, Section~\ref{app:better_error_bound}, and leave the
implementation details to future work.

% Daniel: this is probably less important than other parts of the paper. Hence,
% I put it in as \begin{comment} ... \end{comment} but we can think about
% including it back in if it is important enough. Also, the sub-section that was
% after that, improved error bounds on skew estimation, has been moved to the
% supplementary material. I don't see how we can discuss it in the paper without
% taking up too much space.

\begin{comment}

\subsection{Bounds on the Stationary Distribution}
Bounds on the error of an M-H test imply bounds on the stationary distribution
of the Markov chain under appropriate conditions. Such bounds were derived in
both ~\citet{cutting_mh_2014} and~\citet{icml2014c1_bardenet14}. We include the
result from~\citet{cutting_mh_2014} (Theorem 1) here: Let $d_v(P,Q)$ denote the
total variation distance between two distributions $P$ and $Q$.  Let
$\mathcal{T}_0$ denote the transition kernel of the exact Markov chain,
$\mathcal{S}_0$ denote the exact posterior distribution, and
$\mathcal{S}_{\epsilon}$ denote the stationary distribution of the approximate
transition kernel. 
\begin{lemma}
If $\mathcal{T}_0$ satisfies the contraction condition
$d_v(P\mathcal{T}_0,\mathcal{S}_0) < \eta d_v(P,\mathcal{S}_0)$ for some
constant $\eta\in [0,1)$ and all probability distributions $P$, then
\begin{equation}
      d_v(S_0, S_{\epsilon}) \leq\frac{\epsilon}{1-\eta}
\end{equation}
where $\epsilon$ is the bound on the error in the acceptance test. 
\end{lemma}

\end{comment}



\section{EXPERIMENTS}\label{sec:experiments}

We conduct two sets of experiments to explore the benefits of our minibatch MH
test and to benchmark it with previous work. In Section~\ref{ssec:gaussians}, we
show that our test enables samples to converge to the posterior distribution of
a heated Gaussian mixture model. In Section~\ref{ssec:logistic}, we analyze its
efficiency on logistic regression.

\subsection{Mixture of Gaussians}\label{ssec:gaussians}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{GaussianMixtureResult/posterior_of_gaussian.png}
    \caption{
    The log posterior contours and scatter plots of sampled $\theta$ values
    using different methods. 
    }
    \label{fig:gauss_mix_1}
\end{figure*}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{minibatch_size_gaussian.png}
    \caption{
    Minibatch sizes used in Section~\ref{ssec:gaussians}'s experiment. The axes
    have the same (log-log scale) range.
    }
    \label{fig:gauss_mix_2}
\end{figure*}

We borrow a Gaussian mixture experiment from~\citet{langevin_2011}.  The
parameter is 2-D, $\theta = \langle \theta_1,\theta_2 \rangle$, and the
generation process is
\begin{equation}\label{eq:data_generation}
\begin{split}
    \theta &\sim \mathcal{N}(0, {\rm diag}(\sigma_1^2,\sigma_2^2)) \\
    x_i & \sim 0.5 \cdot \mathcal{N}(\theta_1, \sigma_x^2) + 0.5 \cdot \mathcal{N}(\theta_1+\theta_2, \sigma_x^2).
\end{split}
\end{equation}
We set $\sigma_1^2 = 10, \sigma_2^2 = 1$ and $\sigma_x^2=2$.  We fix $\theta =
\langle 0,1 \rangle$. The original paper sampled 100 data points from this
distribution, and estimated the posterior in the parameters. We are interested
in performance on larger problems and so sampled 1,000,000 points to form the
posterior of $p(\theta)\prod_{i=1}^{1,000,000}p(x_i\mid \theta)$ with the same
prior from Equation~(\ref{eq:data_generation}). This produces a much sharper
posterior with two very narrow peaks.  Our goal is to reproduce the original
posterior, so we adjust the temperature to $T=10,000$.  Taking logs, we get the
target as shown in the far left of Figure~\ref{fig:gauss_mix_1}.

We run MCMC with our M-H test and benchmark with~\citet{cutting_mh_2014}
and~\citet{icml2014c1_bardenet14}, all with initial minibatch size $m=100$.  For
the former, we increment minibatches by 100 and set the tolerance
$\epsilon=0.005$. For the latter, we increase sizes geometrically with a ratio
of $\gamma = 1.5$ and use error parameters $p = 2$ and $\delta = 0.01$.  All
methods collect 5000 samples using the same random walk proposer with covariance
${\rm diag}(0.15, 0.15)$, which means all shaping of distribution of the samples
is due to the M-H test.

Figure~\ref{fig:gauss_mix_1} shows scatter plots of the resulting $\theta$
samples for the three methods, with darker regions indicating a greater density
of points. There are no obvious differences, so we measure the similarity
between each set of samples and the actual posterior. 

We discretize the posterior coordinates into bins with respect to the two
components of $\theta$.  The probability $P_i$ of a sample falling into bin $i$
is the integral of the true posterior probability over the area of that bin.  A
single sample from any of the MH methods should therefore be multinomial with
distribution $P$, and a set of $n$ (ideally independent) samples should be ${\rm
Multinomial}(P,n)$.  The ideal distribution is simple, so we can use it to
measure the quality of the sample distributions rather than use general purpose
tests like KL-divergence or likelihood-ratio, which can be problematic with zero
counts in some bins as we have here.

For large $n$, the per-bin distributions are approximated by Poissons with
parameter $\lambda_i=P_i n$. Given samples $\{\theta_1,\ldots,\theta_T\}$, let
$c_j$ denote the number of individual samples $\theta_i$ that fall in bin $j$
out of $N_{\rm bins}$ total. We have
\begin{equation}\label{eq:log_prob_poissons}
\begin{split}
    \log p(c_1, \ldots, c_{N_{\rm bins}} \mid P_1, \ldots, P_{N_{\rm bins}}) & =  \\
    \sum_{j=1}^{N_{\rm bins}} c_j \log (n P_j) - n P_j - \log(\Gamma(c_j+1)). & \\
\end{split}
\end{equation}
The results for each model are shown in Table~\ref{tab:poissons}. It is
difficult, however, to interpret the scores. Thus, we perform significance tests
to show the difference between the MCMC-sampled distributions and the
ground-truth using the Chi-Squared distribution as the test statistic, which we
also show in Table~\ref{tab:poissons}. Both results imply that our method is
slightly superior to~\citet{cutting_mh_2014}, but slightly worse
than~\citet{icml2014c1_bardenet14}. 

Figure~\ref{fig:gauss_mix_2}, however, suggests that our method dominates in
terms of speed and efficiency. It shows histograms of the (final) minibatch
sizes used each iteration. Our method consumes
significantly less data; most sizes are smaller than 1000, and the
average size is 210.  The other methods occasionally need to consume nearly all
data points, and average minibatch sizes are 15562 and 16857. The average
minibatch sizes roughly predict the running times of these methods since all
have a running time proportional to the total data consumed, with the exception
of~\citet{icml2014c1_bardenet14}, which requires a pass over the data to compute
$C_{\theta,\theta'}$ (see Equation~(\ref{eq:bad_bound})). Using minibatch sizes
as a proxy for time also avoids discrepancies due to code optimization.

\begin{table}[t]
    \caption{Gaussian Mixture Model Statistics}
    \label{tab:poissons}
    \centering
    \resizebox{.45\textwidth}{!}{% <------ Don't forget this %
    \begin{tabular}{l l l l}
    {\bf Metric} & {\bf Ours} & {\bf Korat.'14} & {\bf Barde.'14} \\
    \hline
    Equation~\ref{eq:log_prob_poissons} & -1430.0 & -1578.9 & -1232.7 \\
    Chi-Squared & 3313.9 & 3647.7 & 2444.1 \\
    \end{tabular}% <------ Don't forget this %
   }
\end{table}


\subsection{Logistic Regression}\label{ssec:logistic}

\begin{figure*}[t]
	\centering
	%\includegraphics[width=0.9\linewidth]{logistic_performance.png}
	\includegraphics[width=0.85\linewidth]{logistic_regression_ll_acc_results.png}
	\caption{
    Logistic regression performance (accuracy/log likelihood) based on
    cumulative data usage.
    }
	\label{fig:logistic_performance}
\end{figure*}
\begin{figure*}[t]
	\centering
	%\includegraphics[width=0.9\linewidth]{minibatch_size_logistic.png}
	\includegraphics[width=0.85\linewidth]{logistic_regression_histograms_results.png}
	\caption{
    Counts of minibatch sizes in the MNIST logistic regression experiment
    (analogous to Figure~\ref{fig:gauss_mix_2}).
    }
	\label{fig:logistic_minibatch}
\end{figure*}

We next use logistic regression for the binary classification of 1s versus 7s on
a subset of the MNIST8M dataset, which is a larger version of
MNIST~\citep{lecun-mnisthandwrittendigit-2010} and well-suited to our desire to
run MCMC on large data. We randomly subsampled 450k training and 192k testing
points. The pixels are scaled in $[0,1]$. We impose a flat prior on $\theta$ and
again use a random walk proposer, this time with covariance matrix $0.05I$ for
the $784\times 784$ identity matrix $I$. The posterior temperature is set at
$T=1000$.  We run our MH test for 3000 samples and again compare with the two
baseline algorithms with starting minibatch size 100. To
make~\citet{cutting_mh_2014} run faster, we use a larger $\epsilon=0.05$ value.
For~\citet{icml2014c1_bardenet14}, we tuned $\gamma \in \{1.1,1.25,1.5,1.75,2\}$
before concluding that $\gamma=1.5$ was reasonable. In addition, we originally
used their suggested analytic approximation to $C_{\theta,\theta'}$, but found
that the resulting values were too high, meaning that their method needed to
consume the entire dataset each iteration. Thus, we naively compute
$C_{\theta,\theta'}$.

Figure~\ref{fig:logistic_performance} shows the testing log likelihood and
prediction accuracy, each as a function of the cumulative training data points
processed.\footnote{Note that the curves do not span the same length over the
x-axis, because our test consumes fewer samples throughout the MCMC procedure,
so the corresponding curve will ``end'' before the other two.} To generate the
curves, for each of the 3000 sampled vectors $\theta_t$,
$t\in\{1,\ldots,3000\}$, we use $\theta_t$ as the parameter for logistic
regression.  Our minibatch MH test is more efficient in terms of
accuracy, achieving convergence using roughly half as many elements
compared to~\citet{cutting_mh_2014} and \textbf{TODO TODO TODO} compared
to~\citet{icml2014c1_bardenet14}, though its log likelihood results lag slightly
behind the former algorithm during the very early stages.

Figure~\ref{fig:logistic_minibatch}, in a similar manner as
Figure~\ref{fig:gauss_mix_2}, shows the histogram of minibatch sizes for all
three methods on a log-log scale. With an initial size of 100, our method
achieves an average minibatch size of 393, far smaller than the averages of the
other two methods, which is due to their longer-tailed distributions.



\section{CONCLUSIONS}\label{sec:conclusion}

In this paper, we have derived a new MH test for minibatch MCMC methods. We
demonstrated how a simple deconvolution process allows us to use a minibatch
approximation to the full data tests. We experimentally show the benefits of our
test on Gaussian mixtures and logistic regression.  Straightforward directions
for future work include running more experiments with a particular focus on
investigation of the variance precondition.  More elaborate extensions include
testing on deep neural networks and combining our results with Hamiltonian Monte
Carlo methods, providing a recipe for how to use our algorithm (following the
framework of~\citet{sgmcmc_2015}), or integrating parallel
MCMC~\citep{conf/uai/AngelinoKWSA14,conf/icml/AhnSW14} concepts.





\iffalse
\subsubsection{Figures}

All artwork must be centered, neat, clean, and legible.  All lines
should be very dark for purposes of reproduction, and art work should
not be hand-drawn.  Figures may appear at the top of a column, at the
top of a page spanning multiple columns, inline within a column, or
with text wrapped around them, but the figure number and caption
always appear immediately below the figure.  Leave 2 line spaces
between the figure and the caption. The figure caption is initial caps
and each figure should be numbered consecutively.

Make sure that the figure caption does not get separated from the
figure. Leave extra white space at the bottom of the page rather than
splitting the figure and figure caption.
\begin{figure}[h]
\vspace{.3in}
\centerline{\fbox{This figure intentionally left non-blank}}
\vspace{.3in}
\caption{Sample Figure Caption}
\end{figure}

\subsubsection{Tables}

All tables must be centered, neat, clean, and legible. Do not use hand-drawn tables. Table number and title always appear above the table.
See Table~\ref{sample-table}.

Use one line space before the table title, one line space after the table title, and one line space after the table. The table title must be
initial caps and each table numbered consecutively.

\begin{table}[h]
\caption{Sample Table Title} \label{sample-table}
\begin{center}
\begin{tabular}{ll}
{\bf PART}  &{\bf DESCRIPTION} \\
\hline \\
Dendrite         &Input terminal \\
Axon             &Output terminal \\
Soma             &Cell body (contains cell nucleus) \\
\end{tabular}
\end{center}
\end{table}

\section{SUPPLEMENTARY MATERIAL}

If you need to include additional appendices during submission, you
can include them in the supplementary material file.

\fi


%\newpage
\subsubsection*{References}
% Daniel: Sorry, I don't know why this is needed to get rid of the annoying
% spaces below the "References" text.
\vspace{-20pt} 
%\bibliographystyle{apalike}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,open={[},close={]}}
\bibliography{aistats2017}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\appendix

\begin{center}
{\Large Supplementary Material}
\end{center}

\section{Proof of Lemma~\ref{lem:worst_case}}\label{app:worst_case_proof}

\begin{comment}

{\color{blue} Daniel: changes I made:

\begin{enumerate}
    \item (TODO later) After Equation~\ref{eq:accept_test_zero}, we say that
    ``the resulting test with our chosen $u_0$ will never correctly succeed
    [...]'', but I think we should say it will not succeed with high
    probability, because we could select a minibatch of samples such that the
    mean \emph{actually is} different from 0.5 (it depends on the distribution
    of the variable that corresponds to $Y = \frac{1}{b}\sum_{i=1}^b x_i$).

    \item (TODO later) Unfortunately, I am unable to understand why ``the size
    of the range in $u$ is at least $\exp([-2,-1.125])$'', unless the real range
    is $\exp([-2,0])$ (as I think it should be) and you were simply cropping off
    the extra range.

    \item (TODO later) My main confusion now is that I am unable to derive
    Equation~\ref{eq:log_uo_range}, and we may want to clarify the notation used
    when we put two intervals side by side.

    \item (TODO later) I do not understand why after
    Equation~\ref{eq:accept_test_rewritten}, we say that the standard deviation
    of the LHS ``given the interval constraints is at least $0.5/\sqrt{b}$.'' We
    already know that the $X_i$ are iid $\mathcal{N}(\theta = 0.5, 1)$ variables
    with a fixed $\theta = 0.5$ value, so we already know the standard deviation
    is supposed to be $\sqrt{({\rm Var}(x_1) + \cdots + {\rm Var}(x_b))/b^2} =
    1/\sqrt{b}$. 

    \item I added Equation~\ref{eq:rhs_range}, which I think is how you
    determined the range. Note that it doesn't seem to matter whether it is
    $[-1,0.75]$ or $[-0.75,1]$ since the main thing that matters is that we have
    maximum absolute value of one.

    \item At the end, it said the joint distribution of
    $(\theta,\theta'-\theta,u)$, but I changed it to $(\theta,\theta',u)$, based
    on your change to the text of Lemma~\ref{lem:worst_case}. But here's a
    question: how is the sample of $\theta'$ (or even $\theta'-\theta$, as it
    was written earlier) independent of $\theta$?
\end{enumerate}
}

\end{comment}

Choose $(\theta' - \theta) \in \pm\frac{1}{\sqrt{N}}[0.5,1]$ (event 1) and $(\theta -0.5)
\in \pm\frac{1}{\sqrt{N}}[0.5,1]$ filtered for  matching sign (event 2).  As
discussed in Lemma~\ref{lem:worst_case}, both $q(\theta' \mid \theta)$ and
$p(\theta \mid x_1,\ldots,x_N)$ have variance $1/N$. If we denote $\Phi$ as the
CDF of the standard normal distribution, then the former event occurs with
probability $p_0 = 2(\Phi(\sqrt{N}\frac{1}{\sqrt{N}}) -
\Phi(\sqrt{N}\frac{0.5}{\sqrt{N}})) = 2(\Phi(1)-\Phi(0.5)) \approx 0.2997$. The
latter event, because we restrict signs, occurs with probability $p_1 =
\Phi(1) - \Phi(0.5) \approx 0.14988$. 

These events together guarantee that $\Lambda^*(\theta,\theta')$ is negative
by inspection of equation (\ref{eq:whyneg}) below.
This implies that we can find a $u \in (0,1)$ so that
$\psi(u,\theta,\theta') = \log u < 0$ equals $E[\Lambda^*(\theta,\theta')]$.
Specifically, choose $u_0$ to satisfy $\log u_0 = E[\Lambda^*(\theta,\theta')]$.
Using $E[x_i] = 0.5$ and Equation~(\ref{eq:lemma_ll_ratio}), we see that
\begin{equation}
  \log u_0 = N(\theta'-\theta)\frac{1}{b} \cdot E\left[\sum_{i=1}^b x_i-\theta-\frac{\theta'-\theta}{2}\right]
\end{equation}
\begin{equation}\label{eq:whyneg}
    \log u_0 = -N(\theta'-\theta)\left(\theta-0.5+\frac{\theta'-\theta}{2}\right).
\end{equation}
Next, consider the minibatch acceptance test $\Lambda^*(\theta,\theta') \not\approx
\psi(u,\theta,\theta')$ used in ~\cite{cutting_mh_2014} and~\cite{icml2014c1_bardenet14} , where $\not\approx$
means ``significantly different from'' under the distribution over
samples of $x_i$. This turns out to be
\begin{equation}
\begin{split}
\Lambda^*(\theta,\theta')  \not\approx \psi(u_0,\theta,\theta') & \quad \\
\iff N(\theta'-\theta) \cdot \frac{1}{b}\sum_{i=1}^b x_i-\theta-\frac{\theta'-\theta}{2} & \not\approx \log u_0\\
\iff \frac{1}{b}\sum_{i=1}^b x_i-\left(\theta+\frac{\theta'-\theta}{2} + \frac{\log u_0}{N(\theta'-\theta)}\right) & \not\approx  0 \\
\iff \frac{1}{b}\sum_{i=1}^b x_i-0.5 & \not\approx 0. \label{eq:accept_test_zero}
\end{split}
\end{equation}
Since the $x_i$ have mean 0.5, the resulting test with our chosen $u_0$ will
never correctly succeed and must use all $N$ data points.  Furthermore, if we
sample values of $u$ near enough to $u_0$, the terms in parenthesis will not be
sufficiently different from 0.5 to allow the test to succeed. 
  
The choices above for $\theta$ and $\theta'$ guarantee that
\begin{equation}\label{eq:log_uo_range}
    \log u_0 \in -[0.5,1][0.75,1.5] = [-1.5, -0.375].
\end{equation}
Next, consider the range of $u$ values near $u_0$:
\begin{equation}\label{eq:log_u_range}
    \log u \in \log u_0 + [-0.5,0.375].
\end{equation}
The size of the range in $u$ is at least $\exp([-2,-1.125]) \approx
[0.13534,0.32465]$ and occurs with probability at least $p_2=0.18932$. With $u$
in this range, we rewrite the test as:
\begin{equation}\label{eq:accept_test_rewritten}
    \frac{1}{b}\sum_{i=1}^b x_i-0.5 \hspace{0.1in} \not\approx \hspace{0.1in} \frac{\log u/u_0}{N(\theta'-\theta)}
\end{equation}
so that, as in Equation~(\ref{eq:accept_test_zero}), the LHS has expected value
zero.  Given our choice of intervals for the variables, we can compute the range
for the right hand side (RHS) assuming\footnote{If $\theta'-\theta<0$, then the
range would be $\frac{1}{\sqrt{N}}[-0.75,1]$ but this does not matter for
the purposes of our analysis.} that $\theta'-\theta > 0$:
\begin{equation}\label{eq:rhs_range}
\begin{split}
    \min\{{\rm RHS}\} & = \frac{-0.5}{\sqrt{N} \cdot 0.5} = -\frac{1}{\sqrt{N}} \\
    {\rm and} \quad \max\{{\rm RHS}\} & = \frac{0.375}{\sqrt{N} \cdot 0.5} = \frac{0.75}{\sqrt{N}}
\end{split}
\end{equation}
Thus, the RHS is in $\frac{1}{\sqrt{N}}[-1,0.75]$.  The standard deviation of
the LHS given the interval constraints is at least $0.5/\sqrt{b}$.
Consequently, the gap between the LHS and RHS in
Equation~(\ref{eq:accept_test_rewritten}) is at most $2\sqrt{b/N}$ standard
deviations, limiting the range in which the test will be able to ``succeed''
without requiring more samples.

The samples $\theta$, $\theta'$ and $u$ are drawn independently and so the
probability of the conjunction of these events is $c = p_0 p_1 p_2 = 0.0085$.




\section{Proof of Lemma~\ref{lem:quant_clt}}\label{app:quant_clt}

The following bound is given immediately after Corollary 2 from~\citet{explicit-clt05}:
\begin{align*}
-6.4E|X|^3-2E|X| &\leq \sup_x|{\rm Pr}(t<x)-\Phi(x)|{\sqrt{n}} \\
&\leq 1.36E|X|^3.
\end{align*}
This bound applies to $x\geq 0$. Applying the bound to $-x$ when $x<0$
and combining with $x>0$, we obtain the weaker but unqualified bound
in Equation~(\ref{eq:clt-bounds}).



\section{Proof of Lemma~\ref{lem:cdf_bounds}}\label{app:proof_cdf_bounds}

We first observe that
\[
    P'(z) - Q'(z) = \int_{-\infty}^{+\infty}(P(z-x)-Q(z-x))R(x) dx,
\]
and since $\sup_x|P(x)-Q(x)|\leq \epsilon$ it follows that $\forall z$:
\begin{align*}
-\epsilon &= \int_{-\infty}^{+\infty} -\epsilon R(x) dx \\
&\leq \int_{-\infty}^{+\infty}(P(z-x)-Q(z-x))R(x) dx \\
&\leq \int_{-\infty}^{+\infty}\epsilon R(x) dx = \epsilon,
\end{align*}
as desired.



\section{Proof of Corollary~\ref{cor:bounds_preserved}}\label{app:bounds_preserved}

We apply Lemma~\ref{lem:cdf_bounds} twice. First take:
\begin{equation}
\begin{split}
    P(y) & = {\rm Pr}(\Delta^* < y) \\
     {\rm and} \quad Q(y) & = \Phi\left(\frac{y-\Delta}{s_{\Delta^*}}\right)
\end{split}
\end{equation}
and convolve with the distribution of $X_n$ which has density $\phi(X/\sigma_n)$
where $\sigma_n^2 = 1 - s^2_{\Delta^*}$. This yields the next iteration of $P$
and $Q$:
\begin{equation}
\begin{split}
    P'(y) & = {\rm Pr}(\Delta^*+X_{\rm nc} < y) \\
    \quad {\rm and}\quad Q'(y) & = \Phi\left({y-\Delta}\right)
\end{split}
\end{equation}
Now we convolve with the distribution of $X_{\rm corr}$:
\begin{equation}
\begin{split}
    P''(y) & = {\rm Pr}(\Delta^*+X_{\rm nc}+X_{\rm corr} < y) \\
    \quad {\rm and}\quad Q''(y) & = S\left({y-\Delta}\right)
\end{split}
\end{equation}
Both steps preserve the error bound $\epsilon(\theta,\theta',b)$. Finally
$S(y-\Delta)$ is a logistic CDF centered at $\Delta$, and so $S(y-\Delta) = {\rm
Pr}(\Delta + X_{\rm log} < y)$ for a logistic random $X_{\rm log}$. We conclude
that the probability of acceptance for the actual test ${\rm Pr}(\Delta^*+X_{\rm
nc}+X_{\rm corr} > 0)$ differs from the exact test ${\rm Pr}(\Delta+X_{\rm
log} > 0)$ by at most $\epsilon$.



\section{Improved Error Bounds Based on Skew Estimation}\label{app:better_error_bound}

We demonstrate how we can show $O(n^{-1})$ error for the quantitative CLT by
using a more precise limit distribution under an additional assumption. Let
$\mu_i$ denote the $i^{th}$ moment, and $b_i$ denote the $i^{th}$ absolute
moment of $X$. If Cramer's condition holds:
\begin{equation}\label{eq:cramers_condition}
    \lim_{t \to \infty} \sup |E(\exp(i t X))| < 1,
\end{equation}
then Equation 2.2 in Bentkus et al.'s work on Edgeworth
expansions~\citep{Bentkus97} provides:

\begin{lemma}\label{lem:clt_edgeworth}
Let $X_1,\ldots,X_n$ be a set of zero-mean, independent, identically-distributed
random variables with sample mean $\hat{X}$ and with $t$ defined as in Lemma 3.
If $X$ satisfies Cramer's condition, then
%\begin{equation}\label{eq:clt-bounds_edgeworth}
\[
    \sup_x\left|{\rm Pr}(t<x) - G\left(x, \frac{\mu_3}{b_2^{3/2}}\right)\right| \leq \frac{c(\epsilon,b_2,b_3,b_4,b_{4+\epsilon})}{n}
\]
%\end{equation}
where
\begin{equation}
    G_n(x,y) = \Phi(x) + \frac{y(2x^2+1)}{6\sqrt{n}}\Phi'(x).
\end{equation}
\end{lemma}
Lemma~\ref{lem:clt_edgeworth} shows that the average of the $X_i$ has a more
precise, skewed CDF limit $G_n(x,y)$ where the skew term has weight proportional
to a certain measure of skew derived from the moments:
$\mu_3/b_2^{3/2}$. Note that if the $X_i$ are symmetric, the weight of
the correction term is zero, and the CDF of the average of the $X_i$ converges
to $\Phi(x)$ at a rate of $O(n^{-1})$.

Here the limit $G_n(x,y)$ is a normal CDF plus a correction term that decays as
$n^{-1/2}$.
%{\color{blue} (Daniel: how did the equation in the next sentence
% appear? I verified it and it seems correct, but it looks like it popped out of
%nowhere.)}
% JFC: It comes from taking a second derivative. You dont need to explain it. 
Importantly, since $\phi^{''}(x) = x^2\phi(x) - \phi(x)$ where
$\phi(x)=\Phi'(x)$, the correction term can be rewritten giving:
\begin{equation}\label{eq:GNderivatives}
    G_n(x,y) = \Phi(x) + \frac{y}{6\sqrt{n}}(2\phi^{''}(x)+3\phi(x))
\end{equation}
From which we see that $G_n(x,y)$ is a linear combination of $\Phi(x)$,
$\phi(x)$ and $\phi^{''}(x)$. In Algorithm 1, we
correct for the difference in $\sigma$ between $\Delta^*$ and the variance
needed by $X_{\rm corr}$ using $X_{\rm nc}$. This same method works when we
wish to estimate the error in $\Delta^*$ vs $G_n(x,y)$. Since all of the
component functions of $G_n(x,y)$ are derivatives of a (unit variance)
$\Phi(x)$, adding a normal variable with variance $\sigma'$ increases the
variance of all three functions to $1+\sigma'$. Thus we add $X_{\rm nc}$ as
per Algorithm 1 preserving the limit in Equation~(\ref{eq:GNderivatives}).

The deconvolution approach can be used to construct a correction variable
$X_{\rm corr}$ between $G_n(x,y)$ and $S(x)$ the standard logistic function. An
additional complexity is that $G_n(x,y)$ has additional parameters $y$ and $n$.
Since these act as a single multiplier $\frac{y}{6\sqrt{n}}$ in
Equation~(\ref{eq:GNderivatives}), its enough to consider a function $g(x,y')$
parametrized by $y'= \frac{y}{6\sqrt{n}}$. This function can be computed and
saved offline. As we have shown earlier, errors in the ``limit'' function
propagate directly through as errors in the acceptance test.  To achieve a test
error of $10^{-6}$ (close to single floating point precision), we need a $y'$
spacing of $10^{-6}$. It should not be necessary to tabulate values all the way to
$y'=1$, since $y'$ is scaled inversely by the square root of minibatch size.
Assuming a max $y'$ of 0.1 requires us to tabulate about 100,000.  Since our $x$
resolution is 10,000, this leads to a table with about 1 billion values, which
can comfortably be stored in memory.  However, if $g(x,y)$ is moderately smooth
in $y$, it should be possible to achieve similar accuracy with a much smaller
table. We leave further analysis and experiments with $g(x,y)$ as future work.
%{\color{blue} Daniel: I will look at the code to understand this paragraph
%better because right now I am confused.}
% JFC: the code doesnt implement this yet.




% Daniel: don't forget this. The AISTATS website says to put it in the
% supplementary material, so I put it at the very end on a new page (to make it
% more noticeable). Obviously for the arXiv version, we don't want this there.
% Also, note to John, the reviewers for AISTATS **will have access to the
% previous NIPS reviews**, which is why I am making references to those.
\clearpage
\section{NIPS 2016 Submission Statement}

We previously submitted a much older draft of this paper to NIPS 2016. The
current manuscript has been substantially revised since that submission. We have
made the following changes:

\begin{enumerate}
    \item At the time of the NIPS submission, we were unaware of several
    references which described some of what we wrote. For this submission, we
    now cite the important work of~\citep{TallData15} which had the idea of
    using subsampling noise for exploration (rather than the $\log u$ variable).
    We also cite the work of~\citep{Barker65} which uses the Barker function. We
    welcome information about other potentially missing references.

    \item The previous theoretical results have been entirely scrapped and
    replaced with different but more relevant results. The NIPS submission
    listed a page of theoretical results which did not tie into the rest of the
    paper's contributions and were confusing to the readers. We now have
    theoretical results that are much clearer and also \emph{directly} show the
    expected performance benefits of our acceptance test. Note also that
    Lemma~\ref{lem:worst_case} is an entirely new addition to this version.

    \item Several reviewers mentioned that our deconvolution approach to
    determine the correction distribution was not unique and thus ill-defined.
    Consequently, we have added an entirely new section of the paper
    (Section~\ref{sec:correction}) explaining how we derived that distribution.

    \item Reviewers pointed out one of our mistakes when we said our minibatch
    sizes were fixed. We have changed this in the current manuscript to mean
    that the per-iteration minibatch sizes for our test have a distribution with
    \emph{shorter tails} than those of prior work, which can be observed by
    plotting a histogram of minibatch sizes.

    \item Our experiments now include new comparisons with a baseline
    from~\citep{icml2014c1_bardenet14} (in addition to~\citep{cutting_mh_2014},
    which we had earlier). In addition, each algorithm now runs MCMC sampling on
    the \emph{same} distribution.  Previously, we ran our distribution at a
    higher temperature but kept the algorithm from~\citep{cutting_mh_2014}
    running on the distribution at temperature $T=1$. In this set of
    experiments, we tuned hyperparameters of the other algorithms to make the
    comparison fairer. Finally, in the Gaussian mixture model scenario
    (Section~\ref{ssec:gaussians}), we provide more details on how ``accurate''
    the samples are, rather than solely relying on the visualization of
    Figure~\ref{fig:gauss_mix_1}.

    \item We improved Algorithm~\ref{alg:our_algorithm} so that we show
    explicitly when we compute the moments, and why we now only need one
    distribution $C_\sigma$ for $\sigma=1$ due to the extra $X_{\rm nc}$
    variable.

    \item Finally, we made minor revisions addressing: differences between the
    Barker function vs. the original MH test, and sampling with vs. without
    replacement.
\end{enumerate}

We are confident that the current manuscript is of far better quality than the
NIPS submission, and we appreciate the efforts of the NIPS reviewers to give us
ideas for improvement.

\end{document}
