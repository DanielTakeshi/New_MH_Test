:load simulator.ssc
import org.apache.commons.math3.distribution._

val seed:IMat = loadIMat("seed_lr.imat")
val seed2:IMat = seed(1) to 100
saveIMat("seed_lr.imat", seed2)
val thisSeed = seed(0)
println("\n\tin CUTTING-MH with seed = " +thisSeed+ ".\n")
setseed(thisSeed)


class cutMHTest{
	var N = 100000;  // number of data points
	var eps = 0.005; // error bound
	var error_bound = true; // true means use fixed error bound
	def testfn(diff_in:Mat, u: Double, nsig:Double):(Boolean, Boolean) = {
		// diff: N / temp * \sum_{i=1}^m (log(p(x,ttheta)) - log(p(x,theta)))
		val diff = diff_in / (N * 1.0);
		val diff_sq = diff dot diff;
		val lsq_bar = mean(diff_sq).dv
		val l_bar = mean(diff).dv
		val l_bar_sq = l_bar * l_bar
		
		val n = diff.size     		
		val sl = math.sqrt((lsq_bar - l_bar_sq) * n * 1.0 /(n-1.0));
		val s = sl/math.sqrt(n*1.0) * math.sqrt(1.0 - (n-1.0)/(N-1.0));
		val t = abs( (l_bar  - u)/s );
		val TD = new TDistribution(n-1)
		val tcdf = TD.cumulativeProbability(t.dv)
		val delta = (1.0 - tcdf).dv

		if (delta < nsig || diff.size >= N){
			if( l_bar > u){
				return (true, true)
			}
			else{
				return (true, false)
			}
		}else{
			return (false, false);
		}
	};
};

/** New change: adding mu_0 as an input. */
def cutMHTest_dostep(mod:MHmodel, test:cutMHTest, data:Mat, size:Int, here:Int, 
                     theta:Mat, ttheta:Mat, acc:Double, mu_0:Double):(Int, Mat, Double) = {
	var step = size;   // initial batchsize
	var done = false;
	var ntheta:Mat = null;
	var there = 0;
	var istep = 0;
	var ll:Mat = null;
	var batchsize = size;

	while(!done) {
		val batch = getbatch(mod.data, here, batchsize);
		ll = mod.evalfn(batch, theta);
		val diff = mod.evalfn(batch, ttheta) - ll;
		val nsig = test.eps;
		val (moved, takestep) = test.testfn(diff, mu_0, nsig);
		done = moved;
		if (done) {
			there = (here + batchsize) % data.ncols;
			ntheta = if (takestep) ttheta else theta;
		} else {
			step = min(size, test.N - batchsize).v;
			batchsize = batchsize + step;
		}
		istep += 1;
	}
	(there, ntheta, mean(ll).dv/(test.N*1.0) );
}


/**
 * Helper method for the optimization problem of Korattikara et al, where we
 * binary-search into the generated matlab matrix. Given a mu_std value, we need
 * to find the closest mu_value that exists in our matrix. Here, low and high
 * represent indices, so [mu_std_array(low), ..., mu_std_array(high)]. Note that
 * we include both low and high in this array range. This _should_ be working;
 * I tested it on our mu arrays after borrowing from an online implementation.
 * EDIT: actually, not 100% sure why but we have to add +1 to get the indices to
 * line up.
 *
 * @param mu_std The computed mu_std value, our "target" value.
 * @param mu_std_array The array (well, matrix) of mu_std values.
 * @return The index `i` into the mu_std_array such that mu_std_array(i) is
 *      closer to mu_std than any other element in mu_std_array.
 */
def find_closest(mu_std:Float, mu_std_array:DMat) = {
    var low = 0
    var high = mu_std_array.length - 1
    var i = -1
    while (low < high) {
        i = (low + high) / 2
        val d1 = math.abs(mu_std_array(i) - mu_std)
        val d2 = math.abs(mu_std_array(i+1) - mu_std)
        if (d2 <= d1) { 
            // closest value to mu_std is in the second half
            low = i+1
        } else { 
            // closest value to mu_std is in the first half
            high = i
        }
    }
    i+1
}


/**
 * Performs an optimization problem via grid search over (epsilon,minibatch)
 * size values to determine which epsilon to use. This is their
 * "non-conservative" strategy, which depends on \mu_std.
 *
 * @param mod The MH Test model, which we use to invoke the proposer since we
 *      need the correct proposer to get a reasonable \theta_i sample for the
 *      empirical expectations.
 * @param test The CutMH test model, e.g., so we can get the exact N value.
 * @param path The path to the MATLAB file we need (DIFFERENT from conservative
 *      test).
 * @param deltaStar The constraint on error, so we check that the error we get
 *      from the pre-computed MATLAB code is bounded by this. It follows the the
 *      Korattikara paper notation; it is NOT the Delta used in OUR paper.
 * @param theta The current theta.
 * @param ttheta The proposed theta.
 * @param mu_0 The mu_0 which is log(u)/N really.
 * @return The best (epsilon,minibatch) pairing to use.
 */
def nonConservativeSearch(mod:MHmodel, test:cutMHTest, path:String,
                          deltaStar:Float, theta:Mat, ttheta:Mat, mu_0:Double) = {
    var epsilons = row(0.001, 0.005, 0.01, 0.05, 0.1, 0.2)
    var sizes = irow(50, 100, 150, 200, 250, 300, 350, 400, 450, 500)
    var bestEps = epsilons(0)
    var bestSize = sizes(0)
    var bestDataUsage = test.N.dv  

    // I used the following two names in the non-conservative case:
    val mu_std_values:DMat = load(path, "mu_std_values")
    val mat_err_meanj:DMat = load(path, "result")
    if (mat_err_meanj.ncols != (epsilons.length * sizes.length * 2)) {
        throw new RuntimeException("Error in data dimensions: " +
            "mat_err_meanj.ncols = " +mat_err_meanj.ncols)
    }
    if (mu_std_values.length != mat_err_meanj.nrows) {
        throw new RuntimeException("Error in mu_std and result alignment.")
    }

    // Compute the mu_std value on FULL data!
    val diff = mod.evalfn(mod.data, ttheta) - mod.evalfn(mod.data, theta)
    val stdev_l = sqrt( variance(diff) ).dv
    val mu = sum(diff) / test.N.dv
    val mu_std = (((mu - mu_0) * sqrt(test.N - 1.0).dv) / stdev_l).dv.toFloat

    // Find *index* of closest mu_std in our table. This gives us a 2-D slice.
    val mu_std_index = find_closest(mu_std, mu_std_values)
    val offset = epsilons.length * sizes.length
    val mat_error = mat_err_meanj(mu_std_index, 0 -> offset)
    val mat_meanj = mat_err_meanj(mu_std_index, offset -> 2*offset)

    // Find the best (e,m) pairing; the dataUsage and error are computed here.
    // Don't forget that BIDMach, unlike MATLAB, uses 0-indexing.
    // Formula is: ``` e*(number of m's) + m ```.
    for (e <- 0 until epsilons.length) {
        for (m <- 0 until sizes.length) {

            val dataUsage = mat_meanj(e*sizes.length + m) * sizes(m) 
            val error = mat_error(e*sizes.length + m)

            if (dataUsage < bestDataUsage && error < deltaStar) {
                bestDataUsage = dataUsage
                bestEps = epsilons(e)
                bestSize = sizes(m)
            }
        }
    }

    if (bestDataUsage == test.N.dv) {
        throw new RuntimeException("bestDataUsage is still at " +test.N.dv)
    }
    //println("mu_std="+mu_std+", bestEps="+bestEps+", bestSize="+bestSize)
    (bestEps, bestSize)
}



/**
 * Runs AustereMH. It uses either the conservative or non-conservative way. The
 * only code impact should be at the start of each new iteration when it calls
 * the grid search.
 *
 * @param mod The MH Test model, which we use to invoke the proposer since we
 *      need the correct proposer to get a reasonable \theta_i sample for the
 *      empirical expectations.
 * @param test The CutMH test model, e.g., so we can get the exact N value.
 * @param nsamps The number of "\theta"s to sample.
 * @param size The minibatch size, which is fixed if conservative=True, but if
 *      not, it will generally change often.
 * @param acc No idea, we ignore this ...
 * @param path The path to the MATLAB file we need for getting epsilon and
 *      minibatch sizes each time we get a new \theta.
 * @param deltaStar The constraint on error, so we check that the error we get
 *      from the pre-computed MATLAB code is bounded by this. It follows the the
 *      Korattikara paper notation; it is NOT the Delta used in OUR paper.
 * @param conservative If False, we have to call the (epsilon,minibatch) code to
 *      extract some choice. If True, it's set beforehand and we don't use the
 *      path.
 * @return The (samples, sizes, train log likelihoods, seed) information.
 */
def cutMHTest_dosimm(mod:MHmodel, test:cutMHTest, nsamps:Int, size:Int,
                     acc:Double, path:String, deltaStar:Float,
                     conservative:Boolean):(DMat,LMat,DMat,Int) = {
    var theta = mod.initfn();
    val samples = dzeros(theta.length, nsamps);
    val sizes = lzeros(1, nsamps);
    val lls = dzeros(1, nsamps);
    var here = 0;
    var i = 0;
    var mbSize = size // this will contain sizes
    
    tic;
    println("")
    println("at start, mbSize=" +mbSize+ ", test.eps=" +test.eps)
    while (i < nsamps) {
    	if (i % 100 == 0){
            val t = toc;
    		println("cut mh test iteration = %d, elapsed time = %f secs" format(i, t));
    	}
    	if (! test.error_bound){
    		val epsi = 0.01 * pow((0.01 + i), -0.55).dv;
    		test.eps = epsi;
    	}
		val ttheta = mod.proposalfn(theta);

        // MAJOR NEW CHANGES, to modify eps and size before doing the test step.
        // This _should_ be the correct location ... I'm also putting mu_0.
	    val mu_0 = 1.0/test.N * ln(rand(1,1)).dv;

        if (!conservative) {
            val (thisEps, thisSize) = nonConservativeSearch(mod, test, 
                                                            path, deltaStar, 
                                                            theta, ttheta, mu_0)
            test.eps = thisEps
            mbSize = thisSize
            //println("i="+i+", mbSize=" +mbSize+ ", test.eps=" +test.eps)
        }

		val (there, nth, ll) = cutMHTest_dostep(mod, test, mod.data, mbSize, here, theta, ttheta, acc, mu_0);
		sizes(i) = if (there > here) (there - here) else (there - here + mod.data.ncols);
		lls(i) = ll;
		here = there;
		theta = nth;
		samples(?, i) = DMat(theta);
		i += 1;
    }
    println("")
    (samples, sizes, lls, thisSeed);
};


/**
 * Performs an optimization problem via grid search over (epsilon,minibatch)
 * size values to determine which epsilon to use. This is their "conservative"
 * strategy, which automatically assumes \mu_std = 0. See Equation 8 in their
 * paper. This can be done entirely before we start collecting samples of
 * \theta. Once we set a desired error rate (via deltaStar) there is nothing
 * else for us to check so we can use that same minibatch size and epsilon for
 * each iteration, where "iteration" here is defined as the process of deciding
 * one accept or reject decision.
 * 
 * Call this code _before_ we run the simulator! And you should also ensure that
 * all relevant file names are correct ...
 *
 * @param mod The MH Test model, which we use to invoke the proposer since we
 *      need the correct proposer to get a reasonable \theta_i sample for the
 *      empirical expectations.
 * @param test The CutMH test model, e.g., so we can get the exact N value.
 * @param path The path to the MATLAB file we need.
 * @param deltaStar The constraint on error, so we check that the error we get
 *      from the pre-computed MATLAB code is bounded by this. It follows the the
 *      Korattikara paper notation; it is NOT the Delta used in OUR paper.
 * @return The best (epsilon,minibatch) pairing to use.
 */
def get_conservative_mb_eps(mod:MHmodel, test:cutMHTest, path:String, deltaStar:Float) = {
    // I used these for the conservative case, forming a (6,10) matrix.
    var epsilons = row(0.001, 0.005, 0.01, 0.05, 0.1, 0.2)
    var sizes = irow(50, 100, 150, 200, 250, 300, 350, 400, 450, 500)
    var bestEps = epsilons(0)
    var bestSize = sizes(0)
    var bestDataUsage = test.N.dv  

    val mat_error:DMat = load(path, "result_error")
    val mat_meanj:DMat = load(path, "result_meanj")
    if (mat_error.nrows != epsilons.length || mat_meanj.nrows != epsilons.length) {
        throw new RuntimeException("Error in number of epsilons.")
    }
    if (mat_error.ncols != sizes.length || mat_meanj.ncols != sizes.length) {
        throw new RuntimeException("Error in number of sizes.")
    }

    for (e <- 0 until epsilons.length) {
        for (m <- 0 until sizes.length) {
            val dataUsage = mat_meanj(e,m) * sizes(m)
            if (dataUsage < bestDataUsage && mat_error(e,m) < deltaStar) {
                bestDataUsage = dataUsage
                bestEps = epsilons(e)
                bestSize = sizes(m)
            }
        }
    }

    if (bestDataUsage == test.N.dv) {
        throw new RuntimeException("bestDataUsage is still at " +test.N.dv)
    }
    (bestEps, bestSize)
}
