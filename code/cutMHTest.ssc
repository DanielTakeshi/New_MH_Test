:load simulator.ssc
import org.apache.commons.math3.distribution._

val seed = 00
println("\n\tin CUTTING-MH with seed = " +seed+ ".\n")
setseed(seed)


class cutMHTest{
	var N = 100000;  // number of data points
	var eps = 0.005; // error bound
	var error_bound = true; // true means use fixed error bound
	def testfn(diff_in:Mat, u: Double, nsig:Double):(Boolean, Boolean) = {
		// diff: N / temp * \sum_{i=1}^m (log(p(x,ttheta)) - log(p(x,theta)))
		val diff = diff_in / (N * 1.0);
		val diff_sq = diff dot diff;
		val lsq_bar = mean(diff_sq).dv
		val l_bar = mean(diff).dv
		val l_bar_sq = l_bar * l_bar
		
		val n = diff.size     		
		val sl = math.sqrt((lsq_bar - l_bar_sq) * n * 1.0 /(n-1.0));
		val s = sl/math.sqrt(n*1.0) * math.sqrt(1.0 - (n-1.0)/(N-1.0));
		val t = abs( (l_bar  - u)/s );
		val TD = new TDistribution(n-1)
		val tcdf = TD.cumulativeProbability(t.dv)
		val delta = (1.0 - tcdf).dv

		if (delta < nsig || diff.size >= N){
			if( l_bar > u){
				return (true, true)
			}
			else{
				return (true, false)
			}
		}else{
			return (false, false);
		}
	};
};

def cutMHTest_dostep(mod:MHmodel, test:cutMHTest, data:Mat, size:Int, here:Int, theta:Mat, ttheta:Mat, acc:Double):(Int, Mat, Double) = {
	var step = size;   // initial batchsize
	var done = false;
	var ntheta:Mat = null;
	var there = 0;
	var istep = 0;
	var ll:Mat = null;
	var batchsize = size;

	val u = 1.0/test.N * ln(rand(1,1)).dv;

	while(!done) {
		val batch = getbatch(mod.data, here, batchsize);
		ll = mod.evalfn(batch, theta);
		val diff = mod.evalfn(batch, ttheta) - ll;
		val nsig = test.eps;
		val (moved, takestep) = test.testfn(diff, u, nsig);
		done = moved;
		if (done) {
			there = (here + batchsize) % data.ncols;
			ntheta = if (takestep) ttheta else theta;
		} else {
			step = min(size, test.N - batchsize).v;
			batchsize = batchsize + step;
		}
		istep += 1;
	}
	(there, ntheta, mean(ll).dv/(test.N*1.0) );
}


def cutMHTest_dosimm(mod:MHmodel, test:cutMHTest, size:Int, nsamps:Int, acc:Double):(DMat,LMat,DMat) = {
    var theta = mod.initfn();
    val samples = dzeros(theta.length, nsamps);
    val sizes = lzeros(1, nsamps);
    val lls = dzeros(1, nsamps);
    var here = 0;
    var i = 0;
    
    tic;
    println("")
    while (i < nsamps) {
    	if (i % 100 == 0){
            val t = toc;
    		println("cut mh test iteration = %d, elapsed time = %f secs" format(i, t));
    	}
    	if (! test.error_bound){
    		val epsi = 0.01 * pow((0.01 + i), -0.55).dv;
    		test.eps = epsi;
    	}
		val ttheta = mod.proposalfn(theta);
		val (there, nth, ll) = cutMHTest_dostep(mod, test, mod.data, size, here, theta, ttheta, acc);
		sizes(i) = if (there > here) (there - here) else (there - here + mod.data.ncols);
		lls(i) = ll;
		here = there;
		theta = nth;
		samples(?, i) = DMat(theta);
		i += 1;
    }
    println("")
    (samples, sizes, lls);
};


/**
 * Helper method for the optimization problem of Korattikara et al, where we
 * compute the \mu_std value.
 *
 * @param mod The MH Test model, which we use to compute \mu_std. Note that we
 *      need the _full_ batch of data.
 * @param N the number of training data points total.
 * @param diff The vector of log proposer - log current terms (same as in other
 *      code here).
 * @param ttheta The proposed theta value.
 * @param mu_0 The current mu_0 value (usually it's (log u)/N for us).
 * @param mu_std_array The array (well, matrix) of mu_std values.
 * @return The value mu_std according to their paper.
 */
def find_mu_std(mod:MHmodel, N:Int, diff:Mat, mu_0:Float) = {
    val stdev_l = sqrt( variance(diff) ).dv
    val mu = sum(diff) / N.dv
    var mu_std = (((mu - mu_0) * sqrt(N-1.0).dv) / stdev_l).dv.toFloat
    mu_std
}


/**
 * Helper method for the optimization problem of Korattikara et al, where we
 * binary-search into the generated matlab matrix. Given a mu_std value, we need
 * to find the closest mu_value that exists in our matrix. Here, low and high
 * represent indices, so [mu_std_array(low), ..., mu_std_array(high)]. Note that
 * we include both low and high in this array range. This _should_ be working;
 * I tested it on our mu arrays after borrowing from an online implementation.
 *
 * @param mu_std The computed mu_std value, our "target" value.
 * @param mu_std_array The array (well, matrix) of mu_std values.
 * @return The index `i` into the mu_std_array such that mu_std_array(i) is
 *      closer to mu_std than any other element in mu_std_array.
 */
def find_closest(mu_std:Float, mu_std_array:DMat) = {
    var low = 0
    var high = mu_std_array.length - 1
    var i = -1
    while (low < high) {
        i = (low + high) / 2
        val d1 = math.abs(mu_std_array(i) - mu_std)
        val d2 = math.abs(mu_std_array(i+1) - mu_std)
        if (d2 <= d1) { 
            // closest value to mu_std is in the second half
            low = i+1
        } else { 
            // closest value to mu_std is in the first half
            high = i
        }
    }
    i
}


/**
 * Performs an optimization problem via grid search over (epsilon,minibatch)
 * size values to determine which epsilon to use. This is their "conservative"
 * strategy, which automatically assumes \mu_std = 0. See Equation 8 in their
 * paper. This can be done entirely before we start collecting samples of
 * \theta. Once we set a desired error rate (via deltaStar) there is nothing
 * else for us to check so we can use that same minibatch size and epsilon for
 * each iteration, where "iteration" here is defined as the process of deciding
 * one accept or reject decision.
 * 
 * Call this code _before_ we run the simulator! And you should also ensure that
 * all relevant file names are correct ...
 *
 * @param mod The MH Test model, which we use to invoke the proposer since we
 *      need the correct proposer to get a reasonable \theta_i sample for the
 *      empirical expectations.
 * @param test The CutMH test model, e.g., so we can get the exact N value.
 * @param path The path to the MATLAB file we need.
 * @param deltaStar The constraint on error, so we check that the error we get
 *      from the pre-computed MATLAB code is bounded by this. It follows the the
 *      Korattikara paper notation; it is NOT the Delta used in OUR paper.
 * @return The best (epsilon,minibatch) pairing to use.
 */
def get_conservative_mb_eps(mod:MHmodel, test:cutMHTest, path:String, deltaStar:Float) = {
    // I used these for the conservative case, forming a (6,10) matrix.
    var epsilons = row(0.001, 0.005, 0.01, 0.05, 0.1, 0.2)
    var sizes = irow(50, 100, 150, 200, 250, 300, 350, 400, 450, 500)
    var bestEps = epsilons(0)
    var bestSize = sizes(0)
    var bestDataUsage = test.N.dv  

    val mat_error:DMat = load(path, "result_error")
    val mat_meanj:DMat = load(path, "result_meanj")
    if (mat_error.nrows != epsilons.length || mat_meanj.nrows != epsilons.length) {
        throw new RuntimeException("Error in number of epsilons.")
    }
    if (mat_error.ncols != sizes.length || mat_meanj.ncols != sizes.length) {
        throw new RuntimeException("Error in number of sizes.")
    }

    for (e <- 0 until epsilons.length) {
        for (m <- 0 until sizes.length) {
            val dataUsage = mat_meanj(e,m) * sizes(m)
            if (dataUsage < bestDataUsage && mat_error(e,m) < deltaStar) {
                bestDataUsage = dataUsage
                bestEps = epsilons(e)
                bestSize = sizes(m)
            }
        }
    }

    if (bestDataUsage == test.N.dv) {
        throw new RuntimeException("bestDataUsage is still at " +test.N.dv)
    }
    (bestEps, bestSize)
}
