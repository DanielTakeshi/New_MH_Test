// this script calculates the kl divergence 

def log_f(theta: Mat, X: Mat, N: Int, T: Double): Mat = {
	// This function calculates the probability of p(theta|X), which is the posterior distribution
	// Since we know that 
	// 			p(\theta|X) \propto p(\theta) * p(X|\theta)
	// where p(\theta) is basically a multivariate gaussian distribution 
	// 			p(\theta) = \frac{1}{\sqrt(  (2*pi)^k * |\Sigma| )} * exp(-0.5 * (\theta - \mu)^T * inv(\Sigma) * (\theta - \mu));
	// where \Sigma is the covariance matrix
	// and that p(X|\theta) is the likelihood term where X is basically a mixture of gaussian distribution
	//			p(X|\theta) = 0.5 * \frac{1}{\sqrt(2*pi)*\sigma_x} * exp(-0.5 * (X - \theta_0)^2/\sigma_x^2) + 
	//						  0.5 * \frac{1}{\sqrt(2*pi)*\sigma_x} * exp(-0.5 * (X - \theta_0 - \theta_1)^2/\sigma_x^2)
	// note that here \sigma_x = \sqrt{2} so we have,
	//			p(X|\theta) = 0.25 * \frac{1}{\sqrt(pi)} * exp(-0.25 * (X - \theta_0)^2) + 
	//						  0.25 * \frac{1}{\sqrt(pi)} * exp(-0.25 * (X - \theta_0 - \theta_1)^2)

	// Input : theta: 2 * n matrix. Every column is a sample, and we have n samples 
	// 		   X    : 1 * N vector. Every entry is a data point, and we have N data points
	// 		   N    : Int         . Number of data points
	// 		   T    : Double      . Temperature

	// Output: res  : 1 * n vector. Every entry denotes the log likelihood of that parameter sample

	val (sizet1, sizet2) = size(theta);  // sizet1: 2
										 // sizet2: the number of all the samples

	if (sizet1 != 2){
		println("Dimension of parameters does not match!");
	}

	val scale_and_temp = N * 1.0/ (X.size() * T);

	val inverse_covariance = 0.1\0.0 on 0.0\1.0;
	val prior_constant = 1.0/( 2 * math.Pi * math.sqrt(10.0));
	// val prior = math.log(prior_constant) - 0.5 * (theta.t * inverse_covariance * theta);
	
	
	val prior = getdiag(math.log(prior_constant) - 0.5 * (theta.t*inverse_covariance*theta));
	
	/*
	val (size_prior1, size_prior2) = size(prior_temp);
	var prior = dzeros(size_prior1, 1);
	for(i <- 0 until size_prior1){
		prior(i) = prior_temp(i,i);
	}
	*/
	

	val ll_constant = 1.0 / (4.0 * math.sqrt(math.Pi));

	val L = ll_constant * (  exp(-0.25*( kron(ones(sizet2,1), FMat(X))-theta(0,?).t ) *@ (kron(ones(sizet2,1), FMat(X))-theta(0,?).t))  + exp(-0.25 *( kron(ones(sizet2,1), FMat(X))-theta(0,?).t - theta(1,?).t ) *@( kron(ones(sizet2,1), FMat(X))-theta(0,?).t - theta(1,?).t )) );


	// val L = ll_constant * (exp(-0.25 * (X - theta(0).dv).dot(X - theta(0).dv) ) + exp(-0.25 * (X - theta(0).dv - theta(1).dv).dot(X - theta(0).dv - theta(1).dv) ) );

	val log_likelihood = sum(ln(L), 2) * scale_and_temp;

	val res = prior + log_likelihood;
	return res.t;
};


def estimate_divergence(all_thetas:Mat, d_space:Double, full_data:Mat, temp:Double): (Double, Double) = {
	val (m,n) = size(all_thetas);
	if (m != 2){
		println("Data dimension error!");
	}

	val N = full_data.size;

	val (min_x, max_x) = (-1.5, 2.5);
	val (min_y, max_y) = (-2.5, 3.0);
	var clipped_thetas = dzeros(m,n);
	val min_x_vec = dones(1, n) * min_x;
	val max_x_vec = dones(1, n) * max_x;
	val min_y_vec = dones(1, n) * min_y;
	val max_y_vec = dones(1, n) * max_y;
	clipped_thetas(0,?) = max(min(all_thetas(0,?), max_x_vec), min_x_vec);
	clipped_thetas(1,?) = max(min(all_thetas(1,?), max_y_vec), min_y_vec);

	val k = 10; // resolution of single grid
	val thetas_rounded = d_space/k * round( clipped_thetas/(d_space/k) );
	val dspace_k = d_space/k;

	val x_coords = DMat(irow(0 -> int((max_x - min_x)/d_space+1).v)) * d_space + min_x;
	val y_coords = DMat(irow(0 -> int((max_y - min_y)/d_space+1).v)) * d_space + min_y;
	val num_x = x_coords.size;
	val num_y = y_coords.size;
	val num_x_k = num_x * k;
	val num_y_k = num_y * k;


	var source_distribution = dzeros(num_x, num_y);
/*	val theta1_list = DMat(irow(0->(num_x_k+1)))*(d_space/k) + min_x - d_space/2;
	val theta2_list = DMat(irow(0->(num_y_k+1)))*(d_space/k) + min_y - d_space/2;
	val theta_list = theta1_list on theta2_list;
	val logf_list = log_f(theta_list, full_data, N, temp);
*/


	for(xc <- 0 until num_x){
		println("xc %d", xc*1.0);
		for(yc <- 0 until num_y){
			val theta1 = x_coords(xc);
			val theta2 = y_coords(yc);
			val theta1_list = DMat(irow(0->(k+1)))*(d_space/k) + theta1 - d_space/2;
			val theta2_list = DMat(irow(0->(k+1))) * (d_space/k) + theta2 - d_space/2;
			/*
			for(xc1 <- 0 until (k+1) ) {
				for(yc1 <- 0 until (k+1)) {
					val this_theta = theta1_list(xc1) on theta2_list(yc1);
					dis_list(xc1, yc1) = log_f(this_theta, full_data, N, temp);
				}
			}
			*/
			val this_theta = theta1_list on theta2_list;
			val dis_list = log_f(this_theta, full_data, N, temp);
			source_distribution(xc, yc) = sum(sum(exp(dis_list))).dv;
		}
	}
	println("finished source distribution calculation");

	var target_distribution = dzeros(num_x, num_y);
	for(i <- 0 until n){
		val xt = int( round((thetas_rounded(0,i) - min_x)/d_space)).v;
		val yt = int( round((thetas_rounded(1,i) - min_y)/d_space)).v;
		target_distribution(xt,yt) = target_distribution(xt,yt) + 1;		
	}
	println("finished target distribution calculation");

	source_distribution = source_distribution / sum(sum(source_distribution)).dv * n;
	target_distribution = target_distribution / sum(sum(target_distribution)).dv * n;

	// calculate the chi-square statistics
	val chi_sq = sum(sum( (target_distribution - source_distribution)*@(target_distribution - source_distribution)/source_distribution) ).dv;

	// calculate the Log Poisson Likelihood term 
	val obs = round(target_distribution) + 1;   // observed distribution
	val tho = round(source_distribution) + 1;   // expected distribution
	val logll = sum(sum(obs*@ln(tho) - tho - ln(gamma(obs+1.0)))).dv;

	return (chi_sq, logll);
}


val nn = 1000000;
val data = dzeros(1,nn);
val u = rand(1,nn);
val theta1 = 0.0;
val theta2 = 1.0;
val sigma = math.sqrt(2);
val temp = 10000.0
for(i <- 0 until nn){
		if (u(i) < 0.5){
			data(i) = dnormrnd(theta1, sigma.toFloat,1,1)(0)
		}
		else{
			data(i) = dnormrnd(theta1+theta2, sigma.toFloat,1,1)(0)
		}
};

val samples1:DMat = load("gaussiandata.mat","newtestsamples");
val res1, logll1 = estimate_divergence(samples1, d_space=0.1, data, temp);
println("res1 is %d, and logll1 is %d ", res1, logll1);

// val samples2:DMat = load("gaussiandata.mat","cutmhsamples");
// val res2, logll2 = estimate_divergence(samples2, d_space = 0.1, data, temp);
// println("res2 is %d, and logll2 is %d ", res2, logll2);

// val samples3:DMat = load("gaussiandata.mat", "adaptivemhsamples");
// val res3, logll3 = estimate_divergence(samples3, d_space = 0.1, data, temp);
// println("res3 is %d, and logll3 is %d ", res3, logll3);




