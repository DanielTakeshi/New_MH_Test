// this script calculates the kl divergence 

def log_f(theta: Mat, X: Mat, N: Int, T: Double): Mat = {
	// This function calculates the probability of p(theta|X), which is the posterior distribution
	// Since we know that 
	// 			p(\theta|X) \propto p(\theta) * p(X|\theta)
	// where p(\theta) is basically a multivariate gaussian distribution 
	// 			p(\theta) = \frac{1}{\sqrt(  (2*pi)^k * |\Sigma| )} * exp(-0.5 * (\theta - \mu)^T * inv(\Sigma) * (\theta - \mu));
	// where \Sigma is the covariance matrix
	// and that p(X|\theta) is the likelihood term where X is basically a mixture of gaussian distribution
	//			p(X|\theta) = 0.5 * \frac{1}{\sqrt(2*pi)*\sigma_x} * exp(-0.5 * (X - \theta_0)^2/\sigma_x^2) + 
	//						  0.5 * \frac{1}{\sqrt(2*pi)*\sigma_x} * exp(-0.5 * (X - \theta_0 - \theta_1)^2/\sigma_x^2)
	// note that here \sigma_x = \sqrt{2} so we have,
	//			p(X|\theta) = 0.25 * \frac{1}{\sqrt(pi)} * exp(-0.25 * (X - \theta_0)^2) + 
	//						  0.25 * \frac{1}{\sqrt(pi)} * exp(-0.25 * (X - \theta_0 - \theta_1)^2)

	// Input : theta: 2 * n matrix. Every column is a sample, and we have n samples 
	// 		   X    : 1 * N vector. Every entry is a data point, and we have N data points
	// 		   N    : Int         . Number of data points
	// 		   T    : Double      . Temperature

	// Output: res  : 1 * n vector. Every entry denotes the log likelihood of that parameter sample

	val (sizet1, sizet2) = size(theta);  // sizet1: 2
										 // sizet2: the number of all the samples

	if (sizet1 != 2){
		println("Dimension of parameters does not match!");
	}

	val scale_and_temp = N * 1.0/ (X.size() * T);

	val inverse_covariance = 0.1\0.0 on 0.0\1.0;
	val prior_constant = 1.0/( 2 * math.Pi * math.sqrt(10.0));
	// val prior = math.log(prior_constant) - 0.5 * (theta.t * inverse_covariance * theta);
	// val prior = getdiag(math.log(prior_constant) - 0.5 * (theta.t*inverse_covariance*theta));
	var prior = math.log(prior_constant) - 0.5 * sum((theta*@theta)*@(0.1 on 1),1);
	

	val ll_constant = 1.0 / (4.0 * math.sqrt(math.Pi));

	val L = ll_constant * (  exp(-0.25*( kron(ones(sizet2,1), FMat(X))-theta(0,?).t ) *@ (kron(ones(sizet2,1), FMat(X))-theta(0,?).t))  + exp(-0.25 *( kron(ones(sizet2,1), FMat(X))-theta(0,?).t - theta(1,?).t ) *@( kron(ones(sizet2,1), FMat(X))-theta(0,?).t - theta(1,?).t )) );

	var log_likelihood = sum(ln(L), 2) * scale_and_temp;

	if(prior.nrows == 1){
		prior = prior.t;
	}
	if(log_likelihood.nrows==1){
		log_likelihood = log_likelihood.t;
	}
	val res = prior + log_likelihood;
	return res.t;
};


def estimate_divergence(all_thetas:Mat, d_space:Double, full_data:Mat, temp:Double): (Double, Double) = {
	val (m,n) = size(all_thetas);
	if (m != 2){
		println("Data dimension error!");
	}

	val N = full_data.size;

	val (min_x, max_x) = (-1.5, 2.5);
	val (min_y, max_y) = (-2.5, 3.0);
	var clipped_thetas = dzeros(m,n);
	val min_x_vec = dones(1, n) * min_x;
	val max_x_vec = dones(1, n) * max_x;
	val min_y_vec = dones(1, n) * min_y;
	val max_y_vec = dones(1, n) * max_y;
	clipped_thetas(0,?) = max(min(all_thetas(0,?), max_x_vec), min_x_vec);
	clipped_thetas(1,?) = max(min(all_thetas(1,?), max_y_vec), min_y_vec);

	val k = 5; // resolution of single grid
	val thetas_rounded = d_space/k * round( clipped_thetas/(d_space/k) );
	val dspace_k = d_space/k;

	val x_coords = DMat(irow(0 -> int((max_x - min_x)/d_space+1).v)) * d_space + min_x;
	val y_coords = DMat(irow(0 -> int((max_y - min_y)/d_space+1).v)) * d_space + min_y;
	val num_x = x_coords.size;
	val num_y = y_coords.size;
	val num_x_k = num_x * k;
	val num_y_k = num_y * k;

	// get source distribution
	var source_distribution = dzeros(num_x, num_y);
	var theta1_list = FMat(kron(ones(num_y_k,1),FMat(irow(0->(num_x_k)))*(d_space/k) + min_x - d_space/2));
	theta1_list = theta1_list(find(theta1_list<1000));
	val theta2_list = FMat(kron(ones(num_x_k,1),(FMat(irow(0->(num_y_k)))*(d_space/k) + min_y - d_space/2).t));
	val theta_list = theta1_list.t on theta2_list.t;
	var logf_list = log_f(theta_list(?, 0 until (k*k*num_y)), full_data, N, temp);
	for(i<- 1 until (num_x) ){
		println("i %d", i*1.0);
		logf_list = logf_list \ log_f(theta_list(?, (i*k*k*num_y) until ((i+1)*k*k*num_y) ), full_data, N, temp);
	}
	// val logf_list = (log_f(theta_list, full_data, N, temp));
	val kk = irow(0 until k);

	for(xc <- 0 until num_x){
		println("xc %d", xc*1.0);
		for(yc <-0 until num_y){
			var xk = IMat(kron(ones(k,1), FMat((xc * k + kk)* num_y_k)) );
			xk = xk(find(xk>(-1)));
			val yk = IMat(kron(ones(k,1), FMat(irow((yc*k) until (yc*k + k)).t)));
			val idx = (xk + yk).t;
			source_distribution(xc,yc) = sum(logf_list(idx)).dv;
		}
	}

	println("finished source distribution calculation");

	// find target distribution
	var target_distribution = dzeros(num_x, num_y);
	for(i <- 0 until n){
		val xt = int( round((thetas_rounded(0,i) - min_x)/d_space)).v;
		val yt = int( round((thetas_rounded(1,i) - min_y)/d_space)).v;
		target_distribution(xt,yt) = target_distribution(xt,yt) + 1;		
	}
	println("finished target distribution calculation");

	source_distribution = source_distribution / sum(sum(source_distribution)).dv * n;
	target_distribution = target_distribution / sum(sum(target_distribution)).dv * n;

	// calculate the chi-square statistics
	val chi_sq = sum(sum( (target_distribution - source_distribution)*@(target_distribution - source_distribution)/source_distribution) ).dv;

	// calculate the Log Poisson Likelihood term 
	val obs = round(target_distribution) + 1;   // observed distribution
	val tho = round(source_distribution) + 1;   // expected distribution
	val logll = sum(sum(obs*@ln(tho) - tho - ln(gamma(obs+1.0)))).dv;
	

	return (chi_sq, logll);
}


val nn = 1000000;
val data:DMat = load("gaussiandata.mat","data");
val temp = 10000.0

val samples1:DMat = load("gaussiandata.mat","newtestsamples");
val (res1, logll1) = estimate_divergence(samples1, d_space=0.1, data, temp);
println(res1, logll1);

val samples2:DMat = load("gaussiandata.mat","cutmhsamples");
val (res2, logll2) = estimate_divergence(samples2, d_space = 0.1, data, temp);
println(res2, logll2);

val samples3:DMat = load("gaussiandata.mat", "adaptivemhsamples");
val (res3, logll3) = estimate_divergence(samples3, d_space = 0.1, data, temp);
println(res3, logll3);

