
def normlogrnd(m:Int, n:Int):DMat = {
    val a = dnormrnd(0, 1.4f, m, n);
    a;
};

def newtest(batch:DMat, theta:DMat, ttheta:DMat, evalfn:(DMat, DMat)=>DMat):(Boolean, DMat) = {
    val diff = evalfn(batch, ttheta) - evalfn(batch, theta);
    val tvar = (variance(diff)/length(diff)).v;
    if (tvar >= 1) {
	(false, null);
    } else {
	val x = mean(diff);
	val xn = dnormrnd(0, math.sqrt(1 - tvar), 1, 1);
	val xc = normlogrnd(1,1);
	if ((x + xn + xc).v > 0) {
	    (true, ttheta);
	} else {
	    (true, theta);
	}
    }
};

def oldtest(batch:DMat, theta:DMat, ttheta:DMat, logu:Double, nsig:Double, evalfn:(DMat, DMat)=>DMat):(Boolean, DMat) = {
    val diff = evalfn(batch, ttheta) - evalfn(batch, theta) - logu;
    val tstd = sqrt(variance(diff)/length(diff)).v
    val ndiff = mean(diff).v / tstd;
    if (math.abs(ndiff) < nsig) {
	(false, null);
    } else {
	if (ndiff > 0) {
	    (true, ttheta);
	} else {
	    (true, theta);
	}
    }
};

def getbatch(data:DMat, here:Int, size:Int):DMat = {
    val there = here + size;
    val nthere = math.min(there, data.ncols);
    val iwrap = math.max(0, there - data.ncols);
    val batch0 = data(?, here->nthere);
    val batch = if (iwrap > 0) {
	batch0 \ data(?, 0->iwrap);
    } else {
	batch0;
    }
    batch;
};    

def newstep(data:DMat, size:Int, here:Int, theta:DMat, ttheta:DMat, evalfn:(DMat,DMat)=>DMat):(Int, DMat) = {
    var step = size;
    var done = false;
    var ntheta:DMat = null;
    var there = 0;
    while (! done) {
	val batch = getbatch(data, here, step);
	val (moved, nth) = newtest(batch, theta, ttheta, evalfn);
	done = moved;
	if (done) {
	    there = (here + step) % data.ncols;
	    ntheta = nth;
	} else {
	    step = math.min(step*2, data.ncols);
	    if (step == data.ncols) throw new RuntimeException("New test failed variance condition");
	}
    }
    (there, ntheta);
};

def oldstep(data:DMat, size:Int, here:Int, theta:DMat, ttheta:DMat, nsig:Double, evalfn:(DMat,DMat)=>DMat):(Int, DMat) = {
    var step = size;
    var done = false;
    var ntheta:DMat = null;
    var there = 0;
    var logu = ln(rand(1,1)).v;
    var nsig0 = nsig;
    while (! done) {
	val batch = getbatch(data, here, step);
	val (moved, nth) = oldtest(batch, theta, ttheta, logu, nsig0, evalfn);
	done = moved;
	if (done) {
	    there = (here + step) % data.ncols;
	    ntheta = nth;
	} else {
	    step = math.min(step*2, data.ncols);
	    if (step == data.ncols) nsig0 = 0;
	}
    }
    (there, ntheta);
};

def newsimm(data:DMat, size:Int, nsamps:Int, initfn:() => DMat, proposalfn:(DMat)=>DMat, evalfn:(DMat,DMat)=>DMat):(DMat,LMat) = {
    var theta = initfn();
    val samples = dzeros(theta.length, nsamps);
    val sizes = lzeros(1, nsamps);
    var here = 0;
    var i = 0;
    while (i < nsamps) {
	val ttheta = proposalfn(theta);
	val (there, nth) = newstep(data, size, here, theta, ttheta, evalfn);
	sizes(i) = if (there > here) (there - here) else (there - here + data.ncols);
	here = there;
	theta = nth;
	samples(?, i) = theta;
	i += 1;
    }
    (samples, sizes);
};

def oldsimm(data:DMat, size:Int, nsamps:Int, nsig:Double, initfn:() => DMat, proposalfn:(DMat)=>DMat, evalfn:(DMat,DMat)=>DMat):(DMat,LMat) = {
    var theta = initfn();
    val samples = dzeros(theta.length, nsamps);
    val sizes = lzeros(1, nsamps);
    var here = 0;
    var i = 0;
    while (i < nsamps) {
	val ttheta = proposalfn(theta);
	val (there, nth) = oldstep(data, size, here, theta, ttheta, nsig, evalfn);
	sizes(i) = if (there > here) (there - here) else (there - here + data.ncols);
	here = there;
	theta = nth;
	samples(?, i) = theta;
	i += 1;
    }
    (samples, sizes);
};
    
abstract class MHmodel(val ndim:Int, val n:Int, val sigma:Double, val pscale:Double, val v:Double) {
    val data:DMat;
    def initfn():DMat;
    def proposalfn(theta:DMat):DMat;
    def evalfn(batch:DMat,theta:DMat):DMat;
};

def newsim(mod:MHmodel, size:Int, nsamps:Int):(DMat, LMat) = {
  newsimm(mod.data, size, nsamps, mod.initfn, mod.proposalfn, mod.evalfn);
};	       

def oldsim(mod:MHmodel, size:Int, nsamps:Int, nsig:Double):(DMat, LMat) = {
  oldsimm(mod.data, size, nsamps, nsig, mod.initfn, mod.proposalfn, mod.evalfn);
};

class NormModel(ndim:Int, n:Int, sigma:Double, pscale:Double, v:Double) extends MHmodel(ndim, n, sigma, pscale, v) {
    val data = dnormrnd(0, sigma, ndim, n);
    val sigmat = sigma/math.sqrt(n);
    val sigmap = sigmat*pscale*v/math.sqrt(ndim);
    def initfn():DMat = {
	dnormrnd(0, sigmat, ndim, 1);
    };
    def proposalfn(theta:DMat):DMat = {
	theta + dnormrnd(0, sigmap, ndim, 1);
    };
    def evalfn(batch:DMat,theta:DMat):DMat = {
	val dd = batch - theta;
	-(n / (2 * sigma * sigma)) * (dd dot dd);
    };
};

val nn = new NormModel(ndim=1, n=100000, sigma=1, pscale=1, v=0.1);

tic
//val (samples, sizes) = newsim(nn, 1000, 20000);
val (samples, sizes) = oldsim(nn, 1000, 10000, 2f);
val tt= toc;

val bsize = 100;
val d0 = nn.data(?,0->bsize);
val t0 = nn.initfn();
val t1 = nn.proposalfn(t0);
val dd = nn.evalfn(d0,t1) - nn.evalfn(d0,t0);
val xmean = mean(dd);
val xstd = math.sqrt(variance(dd).v/dd.length);


    


