val n2lsigma = 0.9;
val nn2l = 2000;
val norm2logdata = loadDMat("norm2log%d_20_%2.1f.txt" format (nn2l, n2lsigma));
val n2ld = norm2logdata(?,0) \ cumsum(norm2logdata(?,1));

abstract class MHmodel(val ndim:Int, val n:Int, val sigma:Double, val pscale:Double) {
    val data:Mat;
    def initfn():Mat;
    def proposalfn(theta:Mat):Mat;
    def evalfn(batch:Mat,theta:Mat):Mat;
};

abstract class MHtestType {
    def testfn(diff:Mat, logu:Double, nsig:Double):(Boolean, Boolean);
    var explin:Boolean;
    def discount(P:Double, n:Int):Double;    
    var keepon = true;
};

def normcdf(a:DMat):DMat = {
    0.5 + 0.5 * erf(a / math.sqrt(2));
};

def normcdfinv(a:DMat):DMat = {
    math.sqrt(2) * erfinv(2*a - 1);
};


/**
 * Computes the correction variable X_c.
 *
 * Note: Like other methods in the BIDMat library, (m,n) indicate the size of
 * the matrix to return; just use (1,1) for our experiments.
 */
def normlogrnd(m:Int, n:Int):DMat = {
    val rr = drand(m, n);
    var i = 0;
    while (i < rr.length) {
        val rv = rr.data(i);
        var top = n2ld.nrows;
        var bottom = 0;
        while (top - bottom > 1) {
            val mid = (top + bottom) / 2;
            if (rv > n2ld(mid, 1)) {
                bottom = mid;
            } else {
                top = mid;
            }
        }
        val y0 = n2ld(bottom, 1);
        val y1 = n2ld(math.min(top, n2ld.nrows-1), 1);
        val alpha = if (y1 != y0) ((rv - y0) / (y1 - y0)) else 0.0;
        val x0 = n2ld(bottom, 0);
        val x1 = n2ld(math.min(top, n2ld.nrows-1), 0);
        val newx = alpha * x1 + (1-alpha) * x0;
        rr.data(i) = newx;
        i += 1;
    }
    rr;
};


/** Our major contribution. See `testfn` documentation for details. */
class NewTest extends MHtestType {

    /**
     * This is our major MH test contribution. See Algorithm 1 of the arXiv 2016
     * paper. Returns a boolean (b1,b2) where b1=True indicates we're finished,
     * otherwise need to increase minibatch size and b2=True indicates that we
     * move to ttheta (new theta), otherwise stick with old theta. A couple of
     * notes on terms:
     *
     * > tvar is the sample variance of the minibatch, indicated by
     *      s_{\Delta*}^2 in our arXiv 2016 paper.
     * > targvar is by default 0.9*0.9. This is described as 1 in the paper but
     *      we set it to be smaller, perhaps for numerical stability?
     * > x is the \Delta* term. Yes, after we take the mean it is correct to my
     *      knowledge (so long as the models include the N/T scaling term).
     *      Whew, that's a relief to know ...
     * > ns TODO I don't understand this? According to the println's it's a
     *      measure of standard deviations. It seems like a heuristic for the
     *      "goodness" of a sample. So if it's really large (say 5 as we have
     *      here?) then we know our proposed \theta is really good so let's move
     *      over there right away, etc. TODO double check.
     * > xn is the extra (univariate) normal variable used for convenience.
     * > xc is drawn from the correction distribution, implemented with our
     *      special normlogrnd method.
     *
     * @param diff A vector containing scaled log likelihood ratios for each
     *      element in the minibatch.
     * @param logu The \psi(u,theta,theta') term, which is log(u) for symmetric
     *      proposals that we use.
     * @param nsig
     */
    def testfn(diff:Mat, logu:Double, nsig:Double):(Boolean, Boolean) = {

        val tvar = variance(diff).dv/diff.length; 
        val targvar = n2lsigma * n2lsigma;
        val x = mean(diff).dv;
        val ns = x / math.sqrt(tvar);

        if (math.abs(ns) > 5) {
            if (ns > 0) {
                (true, true);
            } else {
                (true, false);
            }
        } else {
            if (tvar >= targvar) {
                if (nsig == 0) { 
                    // Only happens if minibatch is full data. Also, keepon=True.
                    if (keepon) {
                        println("Warning: New test failed variance condition, var=%f nstd = %f" format (tvar, ns));
                        if (x > 0) {
                            (true, true);
                        } else {
                            (true, false);
                        }
                    } else {
                        throw new RuntimeException("New test failed variance condition, var=%f, nstd = %f" format (tvar, ns));
                    }
                } else {
                    (false, false);
                }
            } else {
                val xn = dnormrnd(0, math.sqrt(targvar - tvar), 1, 1).dv;
                val xc = normlogrnd(1,1).dv;
                if ((x + xn + xc) > 0) {
                    (true, true);
                } else {
                    (true, false);
                }
            }
        }
    };

    var explin = true;
    def discount(p:Double, n:Int):Double = p;
};


// Our old test. Don't use it.
class OldTest extends MHtestType {
    var explin = true;

    def testfn(diff:Mat, logu:Double, nsig:Double):(Boolean, Boolean) = {
        val tstd = math.sqrt(variance(diff).dv/diff.length);
        val ndiff = mean(diff - logu).dv / tstd;
        if (math.abs(ndiff) < nsig) {
            (false, false);
        } else {
            if (ndiff > 0) {
                (true, true);
            } else {
                (true, false);
            }
        }
    };

    def discount(p:Double, n:Int):Double = {
        val r = 0.5;
        p * (1 - r) * math.pow(r, n);
    }
};


/**
 * Obtains a minibatch of data of `size` elements. Technically it doesn't
 * shuffle but that's for performance reasons.
 *
 * @param data The *full* training data.
 * @param here The "starting point" for the minibatch.
 * @param size The number of elements to be included in our minibatch.
 */
def getbatch(data:Mat, here:Int, size:Int):Mat = {
    val there = here + size;
    val nthere = math.min(there, data.ncols);
    val iwrap = math.max(0, there - data.ncols);
    val batch0 = data.colslice(here, nthere, null);
    val batch = if (iwrap > 0) {
        batch0 \ data.colslice(0, iwrap, null);
    } else {
        batch0;
    }
    batch;
};    


/**
 * Performs a step in the simulation, where we check to see if we should accept
 * our proposed theta. Returns (there, ntheta, ll) where `there` is the final
 * minibatch size needed for this process, `ntheta` is the next theta we should
 * add (either theta or ttheta), and `ll` is the final average (possibly
 * tempered) log likelihood with respect to theta (should that be the new theta
 * instead?).
 *
 * @param mod The model we're using, augmented with a proposer function.
 * @param test The MH test we're using (new or old, use the new one).
 * @param data The *full* training data matrix, accessible from the model.
 * @param size The starting minibatch size, and how much we increment (the
 *      latter only if test.explin=false).
 * @param here The "starting point" for the minibatch.
 * @param theta The current theta.
 * @param ttheta The proposed theta.
 * @param acc Used to determine nsig later for our test.
 */
def dostep(mod:MHmodel, test:MHtestType, data:Mat, size:Int, here:Int, theta:Mat, ttheta:Mat, acc:Double):(Int, Mat, Double) = {
    var step = size;
    var done = false;
    var ntheta:Mat = null;
    var there = 0;
    var istep = 0;
    var ll:Mat = null;

    // In our paper, this is \psi(u,theta,theta') for a uniform prior.
    var logu = ln(rand(1,1)).v;

    // Compute log likelihood ratios into `diff`, which is a *vector* containing
    // each term:
    //      (N / (b*T)) * log(p(xi|theta') / p(xi|theta))
    // in the \Lambda*(theta,theta') term in our arXiv 2016 paper (Equation 4),
    // though we didn't list the `T` for temperature in the paper, and also, we
    // don't do the 1/b scaling until we call mean(diff) in test.testfn().

    while (! done) {
        val batch = getbatch(mod.data, here, step);
        ll = mod.evalfn(batch, theta);
        val diff = mod.evalfn(batch, ttheta) - ll;

        // We use discount factor 0.05; test.discount(a,b) just returns `a`.
        val nsig = if (step == data.ncols) 0.0 else (- normcdfinv(drow(test.discount(acc, istep))).dv);
        val (moved, takestep) = test.testfn(diff, logu, nsig);
        done = moved;

        if (done) {
            there = (here + step) % data.ncols;
            ntheta = if (takestep) ttheta else theta;
        } else {
            // Our results in arXiv 2016 used test.explin = false. Thus we just
            // increment by the same amount each time (we used 100).
            step = math.min(if (test.explin) (step*2) else (step + size), data.ncols);
        }
        istep += 1;
    }
    (there, ntheta, mean(ll).dv);
};


/**
 * Runs the entire training step, returning a matrix of samples (rows are the
 * dimension of \theta) along with the corresponding vectors --- or a
 * (1,nsamps)-dimensional matrices --- of (final!) minibatch sizes and training
 * log likelihoods (on that final minibatch) per iteration.
 *
 * The sizes are computed using here and there. Easiest way to think of these is
 * that here represents the starting point and there the ending point, and the
 * distance is the number of samples chosen. However, here continually updates
 * to be where there started, so it may "wrap around" the data matrix so that
 * special case of (there<here) is needed. Downside is that this means the
 * minibatches aren't truly random; however, this is actually what we do in all
 * algorithms in BIDMach so I guess John argued for this tradeoff.
 *
 * @param mod A subclass of the abstract MHModel class, so it has init,
 *      proposal, and evaluation functions. More accurately, it represents
 *      the model we have (Gaussian mixtures, logistic regression, deep
 *      learning, etc.) **augmented** with proposers.
 * @param test The MH test type, either our old one or new one (but use the new
 *      one!). We don't directly use it here but pass it to dostep().
 * @param size The starting minibatch size, and how much we increment (the
 *      latter only if test.explin=false).
 * @param nsamps The number of \thetas to sample (we did 3000 for our AISTATS
 *      submission).
 * @param acc Used to determine nsig later for our test.
 */
def dosimm(mod:MHmodel, test:MHtestType, size:Int, nsamps:Int, acc:Double):(DMat,LMat,DMat) = {
    var theta = mod.initfn();
    val samples = dzeros(theta.length, nsamps);
    val sizes = lzeros(1, nsamps);
    val lls = dzeros(1, nsamps);
    var here = 0;
    var i = 0;

    println("")
    tic;
    while (i < nsamps) {
        if (i % 100 == 0) {
            val t = toc;
            println("test iteration = %d, elapsed time = %f" format(i, t));
        }
        val ttheta = mod.proposalfn(theta);
        val (there, nth, ll) = dostep(mod, test, mod.data, size, here, theta, ttheta, acc);
        sizes(i) = if (there > here) (there - here) else (there - here + mod.data.ncols);
        lls(i) = ll;
        here = there;
        theta = nth;
        samples(?, i) = DMat(theta);
        i += 1;
    }
    println("")
    (samples, sizes, lls);
};


// Daniel: I think a toy test case?
class NormModel(ndim:Int, n:Int, sigma:Double, pscale:Double) extends MHmodel(ndim, n, sigma, pscale) {
    val data = dnormrnd(0, sigma, ndim, n);
    val sigmat = sigma/math.sqrt(n);
    val sigmap = sigmat*pscale/math.sqrt(ndim);
    var temp = 1.0;
    
    def initfn():Mat = {
        dnormrnd(0, sigmat * math.sqrt(temp), ndim, 1);
    };
    
    def proposalfn(theta:Mat):Mat = {
        theta + dnormrnd(0, sigmap * math.sqrt(temp), ndim, 1);
    };
    
    def evalfn(batch:Mat,theta:Mat):Mat = {
        val dd = batch - theta;
        -(n / (2 * sigma * sigma * temp)) * (dd dot dd);
    };
};

/*
val nn = new NormModel(ndim=1, n=100000, sigma=1, pscale=1);
nn.temp = 1;
val newtest = new NewTest;
val oldtest = new OldTest;

tic;
//val (samples, sizes, lls) = dosimm(mod=nn, test=newtest, size=1000, nsamps=20000, acc=0.05);
val t1 = toc;
//val (samples2, sizes2, lls2) = dosimm(mod=nn, test=oldtest, size=1000, nsamps=10000, acc=0.05);
val t2 = toc - t1;
*/
